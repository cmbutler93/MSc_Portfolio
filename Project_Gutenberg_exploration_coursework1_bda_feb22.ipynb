{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02a: First Steps with MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action:** test your python transcriptions on the Declaration of Independence, using a pipeline such as:\n",
    "\n",
    "cat 1.txt | ./mapper.py | sort -k 1 | ./reducer.py\n",
    "\n",
    "and check that the output looks reasonable. What happens if you don’t have the sorting step (“shuffle”) in the pipeline? Can you explain the output in terms of the operation of the reducer code?\n",
    "\n",
    "You may need to give your python scripts the executable mode (permission) bit, by executing a command such as:\n",
    "\n",
    "chmod +x mapper.py reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat 1.txt |python ./mapper.py | sort -k 1 | python ./reducer.py\n",
    "&\t1\n",
    "1776.\t1\n",
    "4,\t1\n",
    "a\t15\n",
    "A\t1\n",
    "abdicated\t1\n",
    "abolish\t1\n",
    "abolishing\t3\n",
    "absolute\t3\n",
    "Absolved\t1\n",
    "abuses\t1\n",
    "accommodation\t1\n",
    "accordingly\t1\n",
    "accustomed.\t1\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action:** using either method, check that the output file’s contents are the same as those from running the local word histogram job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar /usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar   wordcount 1.txt output\n",
    "16/02/16 12:38:49 INFO client.RMProxy: Connecting to ResourceManager at dsm1/158.223.50.51:8032\n",
    "16/02/16 12:38:50 INFO input.FileInputFormat: Total input paths to process : 1\n",
    "16/02/16 12:38:50 INFO mapreduce.JobSubmitter: number of splits:1\n",
    "16/02/16 12:38:50 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1452790691884_0299\n",
    "16/02/16 12:38:51 INFO impl.YarnClientImpl: Submitted application application_1452790691884_0299\n",
    "16/02/16 12:38:51 INFO mapreduce.Job: The url to track the job: http://dsm1:8088/proxy/application_1452790691884_0299/\n",
    "16/02/16 12:38:51 INFO mapreduce.Job: Running job: job_1452790691884_0299\n",
    "16/02/16 12:38:55 INFO mapreduce.Job: Job job_1452790691884_0299 running in uber mode : false\n",
    "16/02/16 12:38:55 INFO mapreduce.Job:  map 0% reduce 0%\n",
    "16/02/16 12:39:00 INFO mapreduce.Job:  map 100% reduce 0%\n",
    "16/02/16 12:39:09 INFO mapreduce.Job:  map 100% reduce 100%\n",
    "16/02/16 12:39:09 INFO mapreduce.Job: Job job_1452790691884_0299 completed successfully\n",
    "16/02/16 12:39:09 INFO mapreduce.Job: Counters: 49\n",
    "\tFile System Counters\n",
    "\t\tFILE: Number of bytes read=9069\n",
    "\t\tFILE: Number of bytes written=228957\n",
    "\t\tFILE: Number of read operations=0\n",
    "\t\tFILE: Number of large read operations=0\n",
    "\t\tFILE: Number of write operations=0\n",
    "\t\tHDFS: Number of bytes read=8275\n",
    "\t\tHDFS: Number of bytes written=6512\n",
    "\t\tHDFS: Number of read operations=6\n",
    "\t\tHDFS: Number of large read operations=0\n",
    "\t\tHDFS: Number of write operations=2\n",
    "\tJob Counters \n",
    "\t\tLaunched map tasks=1\n",
    "\t\tLaunched reduce tasks=1\n",
    "\t\tData-local map tasks=1\n",
    "\t\tTotal time spent by all maps in occupied slots (ms)=2353\n",
    "\t\tTotal time spent by all reduces in occupied slots (ms)=6464\n",
    "\t\tTotal time spent by all map tasks (ms)=2353\n",
    "\t\tTotal time spent by all reduce tasks (ms)=6464\n",
    "\t\tTotal vcore-seconds taken by all map tasks=2353\n",
    "\t\tTotal vcore-seconds taken by all reduce tasks=6464\n",
    "\t\tTotal megabyte-seconds taken by all map tasks=2409472\n",
    "\t\tTotal megabyte-seconds taken by all reduce tasks=6619136\n",
    "\tMap-Reduce Framework\n",
    "\t\tMap input records=8\n",
    "\t\tMap output records=1314\n",
    "\t\tMap output bytes=13430\n",
    "\t\tMap output materialized bytes=9069\n",
    "\t\tInput split bytes=101\n",
    "\t\tCombine input records=1314\n",
    "\t\tCombine output records=642\n",
    "\t\tReduce input groups=642\n",
    "\t\tReduce shuffle bytes=9069\n",
    "\t\tReduce input records=642\n",
    "\t\tReduce output records=642\n",
    "\t\tSpilled Records=1284\n",
    "\t\tShuffled Maps =1\n",
    "\t\tFailed Shuffles=0\n",
    "\t\tMerged Map outputs=1\n",
    "\t\tGC time elapsed (ms)=52\n",
    "\t\tCPU time spent (ms)=1600\n",
    "\t\tPhysical memory (bytes) snapshot=433057792\n",
    "\t\tVirtual memory (bytes) snapshot=2000158720\n",
    "\t\tTotal committed heap usage (bytes)=402653184\n",
    "\tShuffle Errors\n",
    "\t\tBAD_ID=0\n",
    "\t\tCONNECTION=0\n",
    "\t\tIO_ERROR=0\n",
    "\t\tWRONG_LENGTH=0\n",
    "\t\tWRONG_MAP=0\n",
    "\t\tWRONG_REDUCE=0\n",
    "\tFile Input Format Counters \n",
    "\t\tBytes Read=8174\n",
    "\tFile Output Format Counters \n",
    "\t\tBytes Written=6512\n",
    "[cbutl002@dsm1 big_dat]$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output:\n",
    "[cbutl002@dsm1 big_dat]$ hadoop fs -cat output/part-r-00000\n",
    "&\t1\n",
    "--That\t1\n",
    "1776.\t1\n",
    "4,\t1\n",
    "A\t1\n",
    "Absolved\t1\n",
    "Acts\t2\n",
    "Administration\t1\n",
    "Allegiance\t1\n",
    "Alliances,\t1\n",
    "America,\t2\n",
    "And\t1\n",
    "Annihilation,\t1\n",
    "Appropriations\t1\n",
    "Arbitrary\t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without the sorting step, the counter does not work properly, only counting each word a maximum of once.  The words are also printed in the order that they are written in the Declaration of Independence.  The output reflects the code of the reducer and mapper.  The mapper sorts each word in the text into two columns, that of the word, and that of its count (1 per word).  Then, the reducer reads this and goes through to check whether or not the word it reads is the same as a previous word, and if it is, it adds to its count.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action:** Add the second Gutenberg e-text to your dataset, and re-run the streaming MapReduce job with both files as input. (You can either pass multiple -input switches to hadoop, or you can put both e-texts in an HDFS directory of their own, and pass the directory name itself as the -input). What do you expect to happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[cbutl002@dsm1 big_dat]$ hadoop jar /usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar   wordcount 1.txt 2.txt output\n",
    "16/02/16 12:42:29 INFO client.RMProxy: Connecting to ResourceManager at dsm1/158.223.50.51:8032\n",
    "16/02/16 12:42:30 INFO input.FileInputFormat: Total input paths to process : 2\n",
    "16/02/16 12:42:30 INFO mapreduce.JobSubmitter: number of splits:2\n",
    "16/02/16 12:42:30 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1452790691884_0301\n",
    "16/02/16 12:42:30 INFO impl.YarnClientImpl: Submitted application application_1452790691884_0301\n",
    "16/02/16 12:42:30 INFO mapreduce.Job: The url to track the job: http://dsm1:8088/proxy/application_1452790691884_0301/\n",
    "16/02/16 12:42:30 INFO mapreduce.Job: Running job: job_1452790691884_0301\n",
    "16/02/16 12:42:39 INFO mapreduce.Job: Job job_1452790691884_0301 running in uber mode : false\n",
    "16/02/16 12:42:39 INFO mapreduce.Job:  map 0% reduce 0%\n",
    "16/02/16 12:42:44 INFO mapreduce.Job:  map 100% reduce 0%\n",
    "16/02/16 12:42:49 INFO mapreduce.Job:  map 100% reduce 100%\n",
    "16/02/16 12:42:50 INFO mapreduce.Job: Job job_1452790691884_0301 completed successfully\n",
    "16/02/16 12:42:50 INFO mapreduce.Job: Counters: 50\n",
    "\tFile System Counters\n",
    "\t\tFILE: Number of bytes read=19987\n",
    "\t\tFILE: Number of bytes written=356347\n",
    "\t\tFILE: Number of read operations=0\n",
    "\t\tFILE: Number of large read operations=0\n",
    "\t\tFILE: Number of write operations=0\n",
    "\t\tHDFS: Number of bytes read=19211\n",
    "\t\tHDFS: Number of bytes written=13371\n",
    "\t\tHDFS: Number of read operations=9\n",
    "\t\tHDFS: Number of large read operations=0\n",
    "\t\tHDFS: Number of write operations=2\n",
    "\tJob Counters \n",
    "\t\tLaunched map tasks=2\n",
    "\t\tLaunched reduce tasks=1\n",
    "\t\tData-local map tasks=1\n",
    "\t\tRack-local map tasks=1\n",
    "\t\tTotal time spent by all maps in occupied slots (ms)=5551\n",
    "\t\tTotal time spent by all reduces in occupied slots (ms)=2757\n",
    "\t\tTotal time spent by all map tasks (ms)=5551\n",
    "\t\tTotal time spent by all reduce tasks (ms)=2757\n",
    "\t\tTotal vcore-seconds taken by all map tasks=5551\n",
    "\t\tTotal vcore-seconds taken by all reduce tasks=2757\n",
    "\t\tTotal megabyte-seconds taken by all map tasks=5684224\n",
    "\t\tTotal megabyte-seconds taken by all reduce tasks=2823168\n",
    "\tMap-Reduce Framework\n",
    "\t\tMap input records=271\n",
    "\t\tMap output records=3085\n",
    "\t\tMap output bytes=31033\n",
    "\t\tMap output materialized bytes=19993\n",
    "\t\tInput split bytes=202\n",
    "\t\tCombine input records=3085\n",
    "\t\tCombine output records=1456\n",
    "\t\tReduce input groups=1343\n",
    "\t\tReduce shuffle bytes=19993\n",
    "\t\tReduce input records=1456\n",
    "\t\tReduce output records=1343\n",
    "\t\tSpilled Records=2912\n",
    "\t\tShuffled Maps =2\n",
    "\t\tFailed Shuffles=0\n",
    "\t\tMerged Map outputs=2\n",
    "\t\tGC time elapsed (ms)=58\n",
    "\t\tCPU time spent (ms)=2280\n",
    "\t\tPhysical memory (bytes) snapshot=716533760\n",
    "\t\tVirtual memory (bytes) snapshot=2998452224\n",
    "\t\tTotal committed heap usage (bytes)=603979776\n",
    "\tShuffle Errors\n",
    "\t\tBAD_ID=0\n",
    "\t\tCONNECTION=0\n",
    "\t\tIO_ERROR=0\n",
    "\t\tWRONG_LENGTH=0\n",
    "\t\tWRONG_MAP=0\n",
    "\t\tWRONG_REDUCE=0\n",
    "\tFile Input Format Counters \n",
    "\t\tBytes Read=19009\n",
    "\tFile Output Format Counters \n",
    "\t\tBytes Written=13371\n",
    "[cbutl002@dsm1 big_dat]$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[cbutl002@dsm1 big_dat]$ hadoop fs -cat output/part-r-00000\n",
    "\"AS-IS\".\t1\n",
    "\"Defects\".\t1\n",
    "\"PROJECT\t2\n",
    "\"Project\t2\n",
    "\"Project\").\t1\n",
    "\"Right\t1\n",
    "\"Small\t6\n",
    "\"public\t1\n",
    "\"small\t1\n",
    "#2]\t1\n",
    "&\t1\n",
    "(*)\t1\n",
    "(212-254-5093)\t1\n",
    "(72600.2026@compuserve.com);\t1\n",
    "(_)\t1\n",
    "(and\t2\n",
    "(as\t1\n",
    "(if\t2\n",
    "(or\t3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I passed both 1.txt and 2.txt into the mapper and reducer. I thought that the list of words would be longer, and each word would have more entries, based on the two different texts.  One difference between running the job with hadoop was that the wordcounter ordered words first if they began with a capital letter.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04a: Typed Bytes and Sequence Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer: I had a lot of trouble with the him-her typedbytes tasks.  This is my best effort to let you see my progress and understanding of the task. I figured it would be more valuable to try to understand the proper way to use Hadoop input, than to work on the third lab tasks, since they are unrealistically long. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action:** the following segments of code should all be saved in a single file named typedbytes.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load /Users/cmbutler/Documents/BIG_DATA_APPLICATIONS/typedbytes.py\n",
    "\n",
    "\n",
    "%load -r 1-3 ../code/typedbytes/typedbytes.py\n",
    "\n",
    "#! /usr/bin/env python\n",
    "\n",
    "import sys, struct\n",
    "\n",
    "%load -s encode_string ../code/typedbytes/typedbytes.py\n",
    "\n",
    "def encode_string(string):\n",
    "    if isinstance(string, bytearray):\n",
    "        b = string\n",
    "    elif isinstance(string, str):\n",
    "        b = bytearray(string)\n",
    "    elif isinstance(string, unicode):\n",
    "        b = bytearray(string, 'utf-8')\n",
    "    return bytearray(0) + b'\\x07' + struct.pack('>i', len(b)) + b\n",
    "\n",
    "\n",
    "\n",
    "%load -s encode_file ../code/typedbytes/typedbytes.py\n",
    "\n",
    "def encode_file(path):\n",
    "    size = os.path.getsize(path)\n",
    "    fh = open(path, 'rb')\n",
    "    contents = fh.read()\n",
    "    fh.close()\n",
    "    return encode_string(path) + encode_string(contents)\n",
    "\n",
    "\n",
    "\n",
    "%load -s read_typedbytes ../code/typedbytes/typedbytes.py\n",
    "\n",
    "def read_typedbytes(fh, byte):\n",
    "    if byte == 7:\n",
    "        len, = struct.unpack('>i', fh.read(4))\n",
    "        return fh.read(len)\n",
    "\n",
    "\n",
    "\n",
    "%load -s typedbytes ../code/typedbytes/typedbytes.py\n",
    "\n",
    "def typedbytes(fh):\n",
    "    while True:\n",
    "        x = fh.read(1)\n",
    "        if x == '':\n",
    "            break\n",
    "        key = read_typedbytes(fh, ord(x))\n",
    "        x = fh.read(1)\n",
    "        if x == '':\n",
    "            break\n",
    "        value = read_typedbytes(fh, ord(x))\n",
    "        yield (key, value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Action:** write a program which encodes the Project Gutenberg metadata as typedbytes, writing the result to standard output. Test it on a small subset of the metadata (you can look at the binary contents of the result with Unix utilities such as less) and od))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# %load /Users/cmbutler/Documents/BIG_DATA_APPLICATIONS/encoder.py\n",
    "import sys\n",
    "\n",
    "sys.path += ['.']\n",
    "\n",
    "import typedbytes\n",
    "\n",
    "for path in sys.argv[1:]:\n",
    "\n",
    "    sys.stdout.write(str(encode_file(path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is adapted directly from the 04a worksheet.  Using a command line following this structure: \n",
    "find . -name '*.rdf' | xargs python encode.py\n",
    "We can execute the following code:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for path in sys.argv[1:]:\n",
    "    sys.stdout.write(str(typedbytes.encode_file(path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence, this code imports the typed bytes module, with the path speficied.  Then, it takes the arguments given by the user, the file path to the data to be encoded.  From there, it writes to std.out the encoded file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From Kate on forum\n",
    "\n",
    "# %load /Users/cmbutler/Documents/BIG_DATA_APPLICATIONS/encoder2.py\n",
    "import sys, os\n",
    "sys.path += ['.']\n",
    "import typedbytes\n",
    "for d, dirs, files in os.walk('./cache/epub/**'):\n",
    "        for f in files:\n",
    "                sys.stdout.write(str(typedbytes.encode_file(path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is similar, but it can be modified slightly to be run a bit simpler in python.  In the first code, the file path was given and the code runs through each.  In this code, os.walk function grabs data from each file in the specified file path, making it more useful for further editting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action:** generate a Sequence File containing the metadata for e-texts 1 to 9 of Project Gutenberg (for test purposes)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[cbutl002@dsm1 big_dat]$ cat ./pg1.rdf | python encoder.py | hadoop jar /usr/local/hadoop-2.6.0/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar loadtb tb_output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The output:\n",
    "\n",
    "SEQ\u0006/org.apache.hadoop.typedbytes.TypedBytesWritable/org.apache.hadoop.typedbytes.TypedBytesWritable\u0001\u0000*org.apache.hadoop.io.compress.DefaultCodec\u0000\u0000\u0000\u0000·ë6%0\u001d",
    "≥7∆˘HD\u0015‹\n",
    "è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action:** construct this mapper, and test it on a small typed bytes input like:\n",
    "\n",
    "<generate typedbytes> | python mapper.py | od -t x1\n",
    "\n",
    "to check that the output typedbytes are well-formed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the mapper I created for the rdf library data.  It combines the codes of 03b and 04a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "## this is a workaround for Python 2 encoding issues\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "sys.path += ['.']\n",
    "\n",
    "import os, re, codecs\n",
    "\n",
    "from rdflib import Graph\n",
    "from rdflib.term import bind\n",
    "from rdflib.namespace import Namespace, RDF\n",
    "\n",
    "from typedbytes import typedbytes, encode_string\n",
    "\n",
    "## need to do this explicitly in this version of RDFLib\n",
    "DCTERMS = Namespace(u'http://purl.org/dc/terms/')\n",
    "PGTERMS = Namespace(u'http://www.gutenberg.org/2009/pgterms/')\n",
    "\n",
    "bind(DCTERMS.RFC4646, str)\n",
    "\n",
    "for (key, value) in typedbytes(sys.stdin):\n",
    "    g = Graph()\n",
    "    g.parse(data=value)\n",
    "    for x in g.triples((None, RDF['type'], PGTERMS['book'])):\n",
    "        book = x[0]\n",
    "        lang = [x for x in g.triples((book, DCTERMS['language'], None))]\n",
    "        if not lang:\n",
    "            continue\n",
    "        lang = lang[0][2]\n",
    "        language = [x for x in g.triples((lang, RDF['value'], None))]\n",
    "        if not language:\n",
    "            continue\n",
    "        language = language[0][2]\n",
    "        agent = [x for x in g.triples((book, DCTERMS['creator'], None))]\n",
    "        if not agent:\n",
    "            continue\n",
    "        agent = agent[0][2]\n",
    "        author = [x for x in g.triples((agent, PGTERMS['name'], None))]\n",
    "        if not author:\n",
    "            continue\n",
    "        author = author[0][2]\n",
    "        okey = book.toPython().split(\"/\")[-1]\n",
    "        ovalue = str(author.toPython()) + '\\t' + str(language.toPython())\n",
    "        sys.stdout.write(\"%s%s\" % (encode_string(okey), encode_string(ovalue)))\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code reads the typedbytes input and sorts it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action:** run the mapreduce job above on the small subset of the catalogue, and check that the output from the job is as expected. How many mappers are used in the job? (Compare with the one-mapper-per-file when running on the catalogue files directly).\n",
    "\n",
    "**Action:** construct the sequence file for the whole of the metadata catalogue, and run the MapReduce job above on it. How long does it take? If you increase the number of mappers from the default (using e.g. -D mapred.map.tasks=20), does the job finish faster?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[cbutl002@dsm1 big_dat]$ hadoop jar /usr/local/hadoop-2.6.0/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar  -io typedbytes  -inputformat org.apache.hadoop.mapred.SequenceFileInputFormat  -input tb_output1  -file typedbytes.py  -file gutenberg_rdf_lib_mapper.py -mapper \"python gutenberg_rdf_lib_mapper.py\"  -output first_tb_stream\n",
    "\n",
    "16/02/21 18:16:46 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
    "packageJobJar: [typedbytes.py, gutenberg_rdf_lib_mapper.py, /tmp/hadoop-unjar3047207194166653054/] [] /tmp/streamjob7317074224923551048.jar tmpDir=null\n",
    "16/02/21 18:16:48 INFO client.RMProxy: Connecting to ResourceManager at dsm1/158.223.50.51:8032\n",
    "16/02/21 18:16:48 INFO client.RMProxy: Connecting to ResourceManager at dsm1/158.223.50.51:8032\n",
    "16/02/21 18:16:50 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
    "16/02/21 18:16:50 INFO mapreduce.JobSubmitter: number of splits:1\n",
    "16/02/21 18:16:50 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1452790691884_0555\n",
    "16/02/21 18:16:51 INFO impl.YarnClientImpl: Submitted application application_1452790691884_0555\n",
    "16/02/21 18:16:51 INFO mapreduce.Job: The url to track the job: http://dsm1:8088/proxy/application_1452790691884_0555/\n",
    "16/02/21 18:16:51 INFO mapreduce.Job: Running job: job_1452790691884_0555\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "16/02/21 20:38:58 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
    "packageJobJar: [typedbytes.py, gutenberg_rdf_lib_mapper.py, /tmp/hadoop-unjar6324909998746907731/] [] /tmp/streamjob5033706135182734670.jar tmpDir=null\n",
    "16/02/21 20:39:00 INFO client.RMProxy: Connecting to ResourceManager at dsm1/158.223.50.51:8032\n",
    "16/02/21 20:39:01 INFO client.RMProxy: Connecting to ResourceManager at dsm1/158.223.50.51:8032\n",
    "16/02/21 20:39:07 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
    "16/02/21 20:39:07 INFO mapreduce.JobSubmitter: number of splits:1\n",
    "16/02/21 20:39:08 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1452790691884_0557\n",
    "16/02/21 20:39:09 INFO impl.YarnClientImpl: Submitted application application_1452790691884_0557\n",
    "16/02/21 20:39:09 INFO mapreduce.Job: The url to track the job: http://dsm1:8088/proxy/application_1452790691884_0557/\n",
    "16/02/21 20:39:09 INFO mapreduce.Job: Running job: job_1452790691884_0557\n",
    "16/02/21 20:43:02 INFO mapreduce.Job: Job job_1452790691884_0557 running in uber mode : false\n",
    "16/02/21 20:43:02 INFO mapreduce.Job:  map 0% reduce 0%\n",
    "16/02/21 20:43:10 INFO mapreduce.Job:  map 100% reduce 0%\n",
    "16/02/21 20:43:23 INFO mapreduce.Job:  map 100% reduce 100%\n",
    "16/02/21 20:43:24 INFO mapreduce.Job: Job job_1452790691884_0557 completed successfully\n",
    "16/02/21 20:43:25 INFO mapreduce.Job: Counters: 49\n",
    "\tFile System Counters\n",
    "\t\tFILE: Number of bytes read=6\n",
    "\t\tFILE: Number of bytes written=218205\n",
    "\t\tFILE: Number of read operations=0\n",
    "\t\tFILE: Number of large read operations=0\n",
    "\t\tFILE: Number of write operations=0\n",
    "\t\tHDFS: Number of bytes read=258\n",
    "\t\tHDFS: Number of bytes written=0\n",
    "\t\tHDFS: Number of read operations=7\n",
    "\t\tHDFS: Number of large read operations=0\n",
    "\t\tHDFS: Number of write operations=2\n",
    "\tJob Counters \n",
    "\t\tLaunched map tasks=1\n",
    "\t\tLaunched reduce tasks=1\n",
    "\t\tRack-local map tasks=1\n",
    "\t\tTotal time spent by all maps in occupied slots (ms)=6327\n",
    "\t\tTotal time spent by all reduces in occupied slots (ms)=2744\n",
    "\t\tTotal time spent by all map tasks (ms)=6327\n",
    "\t\tTotal time spent by all reduce tasks (ms)=2744\n",
    "\t\tTotal vcore-seconds taken by all map tasks=6327\n",
    "\t\tTotal vcore-seconds taken by all reduce tasks=2744\n",
    "\t\tTotal megabyte-seconds taken by all map tasks=6478848\n",
    "\t\tTotal megabyte-seconds taken by all reduce tasks=2809856\n",
    "\tMap-Reduce Framework\n",
    "\t\tMap input records=0\n",
    "\t\tMap output records=0\n",
    "\t\tMap output bytes=0\n",
    "\t\tMap output materialized bytes=6\n",
    "\t\tInput split bytes=93\n",
    "\t\tCombine input records=0\n",
    "\t\tCombine output records=0\n",
    "\t\tReduce input groups=0\n",
    "\t\tReduce shuffle bytes=6\n",
    "\t\tReduce input records=0\n",
    "\t\tReduce output records=0\n",
    "\t\tSpilled Records=0\n",
    "\t\tShuffled Maps =1\n",
    "\t\tFailed Shuffles=0\n",
    "\t\tMerged Map outputs=1\n",
    "\t\tGC time elapsed (ms)=69\n",
    "\t\tCPU time spent (ms)=960\n",
    "\t\tPhysical memory (bytes) snapshot=426844160\n",
    "\t\tVirtual memory (bytes) snapshot=2010636288\n",
    "\t\tTotal committed heap usage (bytes)=402653184\n",
    "\tShuffle Errors\n",
    "\t\tBAD_ID=0\n",
    "\t\tCONNECTION=0\n",
    "\t\tIO_ERROR=0\n",
    "\t\tWRONG_LENGTH=0\n",
    "\t\tWRONG_MAP=0\n",
    "\t\tWRONG_REDUCE=0\n",
    "\tFile Input Format Counters \n",
    "\t\tBytes Read=165\n",
    "\tFile Output Format Counters \n",
    "\t\tBytes Written=0\n",
    "16/02/21 20:43:25 INFO streaming.StreamJob: Output directory: first_tb_stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This created an empty file.  I don't quite understand how, because the code was taken from the worksheet.  If anything, I think that the encoder isn't taking the input correctly, or there is some issue with the input.  I'm going to try to continue without the necessary data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04b: Sequence Files for His and Hers Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to adapt the mapper and reducer from your previous attempts. Differences include:\n",
    "\n",
    "1.    we now have the file path as the MapReduce key in the mapper, so we do not need to interrogate the environment for mapreduce_map_input_file;\n",
    "2.    the output from the mapper and the reducer will need to be encoded as typed bytes, as with the mapper in the previous worksheet;\n",
    "3.    both the mapper and the reducer will need to accept input as typed bytes, and iterate over all the given key, value pairs.\n",
    "\n",
    "**Action:** adapt the mapper and reducer previously developed to handle typedbytes input/output. You should be able to test your mapper and reducer by producing some typedbytes data as input to a shell pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the mapper\n",
    "\n",
    "#! /usr/bin/env python\n",
    "\n",
    "import sys, os, re, codecs\n",
    "sys.path += ['.']\n",
    "\n",
    "c = {}; catalog = codecs.open('catalog.dat','r','utf8')\n",
    "for line in catalog:\n",
    "    x = line.strip().split(\"\\t\")\n",
    "    c[x[0]] = (x[1], x[2])\n",
    "\n",
    "#input = os.environ['mapreduce_map_input_file'].split(\"/\")[-1].split(\".\")[0]\n",
    "for (key, value) in typedbytes(sys.stdin):\n",
    "    try:\n",
    "        him = 0; her = 0\n",
    "        for line in sys.stdin:\n",
    "            him += len(re.findall(r'\\b(he|him|himself|his)\\b', line))\n",
    "            her += len(re.findall(r'\\b(she|her|herself|hers)\\b', line))\n",
    "    except:\n",
    "        None\n",
    "    else:\n",
    "        if c[input][1] != \"en\":\n",
    "            None\n",
    "\n",
    "sys.stdout = codecs.getwriter('utf-8')(sys.stdout)\n",
    "    \n",
    "ovalue = str(him.toPython()) + '\\t' + str(her.toPython())\n",
    "sys.stdout.write(\"%s\\t%s\" % (typedbytes.encode_string(c[key][0]), typedbytes.encode_string(ovalue)))\n",
    "sys.stdout.flush()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without the author catalogue data, it is impossible to check this sort of psuedocode.  So, I'll just explain my reasoning.\n",
    "\n",
    "I kept the same code to run through each line in the author catalog.  The value of interest will still be in the same format as before.  Then, I modified the code so that it iterates through each key,value pair in typedbytes(sys.stdin).  I kept the same code for the loop to try, assuming there is no Unicode error.  If there is a unicode error, I didn't want to end the process, so I let it continue through the loop process.\n",
    "\n",
    "Then, I write the counter data to sys.stdout with utf-8 encoding, as before.\n",
    "\n",
    "The print statement needed to be modified to be in typedbytes. I kept the code from the mapper from the 04a labsheet, modifying it slightly for this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The reducer\n",
    "\n",
    "#! /usr/bin/env python\n",
    "\n",
    "import sys, os, re, codecs\n",
    "sys.path += ['.']\n",
    "\n",
    "current_M_word = None; current_M_count = 0; M_word = ''\n",
    "current_F_word = None; current_F_count = 0; F_word = ''\n",
    "for key, value in typedbyes(sys.stdin):\n",
    "    (word, count) = line.split('\\t', 1)\n",
    "    if word == re.(r'\\b(he|him|himself|his)\\b', line)\n",
    "        if current_M_word == current_M_word:\n",
    "            M_count = int(M_count)\n",
    "            current_M_count += M_count\n",
    "        else:\n",
    "            if current_M_word:\n",
    "                print \"%s\\t%s\" % (current_M_word, current_M_count)\n",
    "                current_M_count = M_count\n",
    "                current_M_word = M_word\n",
    "        if M_word == current_M_word:\n",
    "            print \"%s\\t%s\" % (M_word, M_current_count)\n",
    "            sys.stdout.write(\"%s\\t%s\" % (typedbytes.encode_string(c[key][0]), typedbytes.encode_string(ovalue)))\n",
    "    if  word == re.(r'\\b(she|her|herself|hers)\\b', line)\n",
    "        if current_F_word == current_F_word:\n",
    "            F_count = int(M_count)\n",
    "            current_F_count += F_count\n",
    "        else:\n",
    "            if current_F_word:\n",
    "                print \"%s\\t%s\" % (current_F_word, current_F_count)\n",
    "                current_F_count = F_count\n",
    "                current_F_word = F_word\n",
    "        if F_word == current_F_word:\n",
    "            print \"%s\\t%s\" % (F_word, F_current_count)\n",
    "            sys.stdout.write(\"%s\\t%s\" % (typedbytes.encode_string(c[key][0]), typedbytes.encode_string(ovalue)))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I designed the reducer similarly to the wordcount reducer.  I figured, essentially the same process will be happening, except it is necessary to check the word type, i.e. whether the word is a male or female pronoun.  \n",
    "\n",
    "So, I read in the std.in typedbytes data, and split the data into it's proper columns.  Then, I have an if statement to check if the word is male or female oriented.  From there, I compute the counts of each words in the same way for each as it has been done with the wordcount reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with a catalogue file, typed bytes-aware mapper and reducer, and an encoded Sequence File of Project Gutenberg data, we should be in a position to run a MapReduce job. You will need to pass -file arguments not just for the mapper and reducer, but also for the typedbytes.py module and for the catalogue data file, as well as requesting -io typedbytes and -inputformat org.apache.hadoop.mapred.SequenceFileInputFormat. \n",
    "\n",
    "**Action:** run this on a subset of the data first, getting a sense of how long it takes, and then estimate an appropriate number of mappers to run for the full job. You may need to use -D mapred.child.java.opts=-Xmx1024M in order to complete the job on even the largest Project Gutenberg e-text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wish I had gotten to do this task! I'll keep trying!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

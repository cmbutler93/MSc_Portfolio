{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 05b: Social Network (graph) analytics\n",
    "\n",
    "Preliminary data exploration on Reddit data using graphical visualizations. \n",
    "\n",
    "Note: I skipped over the first sections of lab 05b for a few reasons. First, I used the link to the reddit comment data that was placed on the forum instead of the transmission client described in the notebook because it had issues maintaining connection. I did not want to redo the creation of the table 'rc' because I had already completed this step in the lab, so the table was already saved in my HiveQL reddit_comments_feb23 database. I did however repeatedly use the \"ADD JAR /home/cbutl002/Hive-JSON-Serde/json-serde/target/json-serde-1.3.8-SNAPSHOT-jar-with-dependencies.jar;\" line to be able to read the reddit comments.  The following is my subsetting and analysis of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['pylab']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "import pylab\n",
    "import igraph\n",
    "import matplotlib\n",
    "import pandas\n",
    "import numpy\n",
    "import graphistry\n",
    "graphistry.register(key='a16918d5aaa30201ed0bbba1fc70a7e561b7740ca4713119165dd77011f68f29855fdced90924fa70aa4966625dfafad')\n",
    "%matplotlib inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%qtconsole --colors=Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G = igraph.Graph.Read_Ncol('rcreply.ncol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To register with the Python module Graphistry, I had to request a key to the server to create each plot.  However, when I tried to plot this graph using Graphistry as opposed to iGraph, but the graph was so large that it crashed the Graphistry server!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: \"node\" is unbound, automatically binding it to \"__nodeid__\".\n",
      "WARNING: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading 32028 kB. This may take a while...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Large graph: |nodes| + |edges| = 3983189. Layout/rendering might be slow.\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "502 Server Error: Bad Gateway for url: http://labs.graphistry.com/etl?key=a16918d5aaa30201ed0bbba1fc70a7e561b7740ca4713119165dd77011f68f29855fdced90924fa70aa4966625dfafad&usertag=10c6cb5f-pygraphistry-0.9.25&agent=pygraphistry&apiversion=1&agentversion=0.9.25",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e5927896dcb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgraphistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'G.indegree'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'G.outdegree'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/cmbutler/miniconda3/lib/python3.4/site-packages/graphistry/plotter.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, graph, nodes, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mapi_version\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyGraphistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_etl1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mapi_version\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vgraph'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cmbutler/miniconda3/lib/python3.4/site-packages/graphistry/pygraphistry.py\u001b[0m in \u001b[0;36m_etl1\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    282\u001b[0m         response = requests.post(PyGraphistry._etl_url('json'), out_file.getvalue(),\n\u001b[1;32m    283\u001b[0m                                  headers=headers, params=params)\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mjres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cmbutler/miniconda3/lib/python3.4/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 502 Server Error: Bad Gateway for url: http://labs.graphistry.com/etl?key=a16918d5aaa30201ed0bbba1fc70a7e561b7740ca4713119165dd77011f68f29855fdced90924fa70aa4966625dfafad&usertag=10c6cb5f-pygraphistry-0.9.25&agent=pygraphistry&apiversion=1&agentversion=0.9.25"
     ]
    }
   ],
   "source": [
    "graphistry.bind(source='G.indegree', destination='G.outdegree').plot(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of vertices: 805707  number of edges: 3177482\n"
     ]
    }
   ],
   "source": [
    "print(\"number of vertices:\",len(G.vs),\" number of edges:\", len(G.es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max in-degree: 2204, max out-degree: 828\n"
     ]
    }
   ],
   "source": [
    "print(\"max in-degree: %s, max out-degree: %s\" % (G.maxdegree(mode='IN'), G.maxdegree(mode='OUT')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vmaxin = G.vs.select(_indegree=G.maxdegree(mode='IN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vmaxout = G.vs.select(_outdegree=G.maxdegree(mode='OUT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum-degree commenters: ['autowikibot'], ['AutoModerator']\n"
     ]
    }
   ],
   "source": [
    "print(\"maximum-degree commenters: %s, %s\" % (vmaxin[\"name\"], vmaxout[\"name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x192a9fba8>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEHCAYAAABCwJb2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nESV9EZE48v+LTJJKwQ8xAwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x192a9f240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hi = G.degree_distribution(mode='IN')\n",
    "ho = G.degree_distribution(mode='OUT')\n",
    "pylab.xscale('log')\n",
    "pylab.yscale('log')\n",
    "pylab.plot([x[0] for x in hi.bins()], [x[2] for x in hi.bins()])\n",
    "pylab.plot([x[0] for x in ho.bins()], [x[2] for x in ho.bins()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lab 06a: Graph analytics of subreddit comments\n",
    "\n",
    "I will:\n",
    "\n",
    "-> use subqueries to select subsets of the reddit comments to generate parent-child relationships.\n",
    "\n",
    "-> alter configuration parameters of the Hive shell and execution engine.\n",
    "\n",
    "-> use partitions to store a number of subsets of the data for easy later use in external tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subsetting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hive> insert overwrite local directory '/home/cbutl002/rcbigdata'                   \n",
    "    > row format delimited fields terminated by '\\t'\n",
    "    > select a.author, b.author, count(*) from\n",
    "    > (select * from rc where subreddit = 'bigdata') a\n",
    "    > join\n",
    "    > (select * from rc where subreddit ='bigdata') b\n",
    "    > on a.name = b.parent_id\n",
    "    > group by a.author, b.author;\n",
    "Query ID = cbutl002_20160328155959_aeb35434-1257-45a9-b0e5-99b1bb5d549a\n",
    "Total jobs = 2\n",
    "Stage-1 is selected by condition resolver.\n",
    "Launching Job 1 out of 2\n",
    "Number of reduce tasks not specified. Estimated from input data size: 22\n",
    "In order to change the average load for a reducer (in bytes):\n",
    "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
    "In order to limit the maximum number of reducers:\n",
    "  set hive.exec.reducers.max=<number>\n",
    "In order to set a constant number of reducers:\n",
    "  set mapreduce.job.reduces=<number>\n",
    "Starting Job = job_1457314320880_0033, Tracking URL = http://dsm1:8088/proxy/application_1457314320880_0033/\n",
    "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1457314320880_0033\n",
    "Hadoop job information for Stage-1: number of mappers: 41; number of reducers: 22\n",
    "2016-03-28 15:59:12,941 Stage-1 map = 0%,  reduce = 0%\n",
    "2016-03-28 15:59:24,503 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 152.75 sec\n",
    "2016-03-28 15:59:26,602 Stage-1 map = 2%,  reduce = 0%, Cumulative CPU 314.96 sec\n",
    "2016-03-28 15:59:28,769 Stage-1 map = 3%,  reduce = 0%, Cumulative CPU 454.76 sec\n",
    "2016-03-28 15:59:29,831 Stage-1 map = 4%,  reduce = 0%, Cumulative CPU 528.57 sec\n",
    "2016-03-28 15:59:31,926 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 628.16 sec\n",
    "#\n",
    "# ...\n",
    "#\n",
    "2016-03-28 16:01:53,140 Stage-1 map = 100%,  reduce = 32%, Cumulative CPU 6604.72 sec\n",
    "2016-03-28 16:01:54,166 Stage-1 map = 100%,  reduce = 63%, Cumulative CPU 6615.58 sec\n",
    "2016-03-28 16:01:55,185 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6631.39 sec\n",
    "MapReduce Total cumulative CPU time: 0 days 1 hours 50 minutes 31 seconds 390 msec\n",
    "Ended Job = job_1457314320880_0033\n",
    "Launching Job 2 out of 2\n",
    "Number of reduce tasks not specified. Estimated from input data size: 1\n",
    "In order to change the average load for a reducer (in bytes):\n",
    "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
    "In order to limit the maximum number of reducers:\n",
    "  set hive.exec.reducers.max=<number>\n",
    "In order to set a constant number of reducers:\n",
    "  set mapreduce.job.reduces=<number>\n",
    "Starting Job = job_1457314320880_0034, Tracking URL = http://dsm1:8088/proxy/application_1457314320880_0034/\n",
    "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1457314320880_0034\n",
    "Hadoop job information for Stage-2: number of mappers: 6; number of reducers: 1\n",
    "2016-03-28 16:02:01,751 Stage-2 map = 0%,  reduce = 0%\n",
    "2016-03-28 16:02:05,866 Stage-2 map = 50%,  reduce = 0%, Cumulative CPU 2.45 sec\n",
    "2016-03-28 16:02:06,892 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 5.37 sec\n",
    "2016-03-28 16:02:10,994 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 6.63 sec\n",
    "MapReduce Total cumulative CPU time: 6 seconds 630 msec\n",
    "Ended Job = job_1457314320880_0034\n",
    "Copying data to local directory /home/cbutl002/rcbigdata\n",
    "Copying data to local directory /home/cbutl002/rcbigdata\n",
    "MapReduce Jobs Launched: \n",
    "Stage-Stage-1: Map: 41  Reduce: 22   Cumulative CPU: 6631.39 sec   HDFS Read: 5462381833 HDFS Write: 4086 SUCCESS\n",
    "Stage-Stage-2: Map: 6  Reduce: 1   Cumulative CPU: 6.63 sec   HDFS Read: 10566 HDFS Write: 1152 SUCCESS\n",
    "Total MapReduce CPU Time Spent: 0 days 1 hours 50 minutes 38 seconds 20 msec\n",
    "OK\n",
    "Time taken: 186.668 seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action:** describe in words what you expect this query to produce, and where. Run it (altering the youid001 to your userid) and see if the output is what you expect. You may need to USE 05b first.\n",
    "\n",
    "I would expect this to produce a new tab-delimited data table in a folder called rcbig data.  It would be a joined dataset from two tables a and b, which are both the full selection of the a and b tables taken from the bigdata subreddit.  It would count the number of authors who are parent authors of a comment in a subthread.\n",
    "\n",
    "In reality, this command seems to have made a few datasets which list the username of each user, the threads that they have replied on, and the count of the replies.\n",
    "\n",
    "**Action:** choose another subreddit, maybe one that is bigger (e.g. leagueoflegends), and rerun the query (alter the destination directory). Does the query take any longer to run?\n",
    "\n",
    "I chose to run the command on the leagueoflegends subreddit.  The bigdata subreddit MapReduce job took about 3 minutes to run, and the leagueoflegends subreddit MapReduce job took about 3 minutes to run, as well.  The input and output is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hive> insert overwrite local directory '/home/cbutl002/leagueoflegends'             > row format delimited fields terminated by '\\t'                                > select a.author, b.author, count(*) from                                      > (select * from rc where subreddit = 'leagueoflegends') a                      > join                                                                          > (select * from rc where subreddit = 'leagueoflegends') b                      > on a.name = b.parent_id                                                       > group by a.author, b.author; \n",
    "Query ID = cbutl002_20160328161818_aaf1d655-1b9f-425a-b6c0-86495b4a5543\n",
    "Total jobs = 2\n",
    "Stage-1 is selected by condition resolver.\n",
    "Launching Job 1 out of 2\n",
    "Number of reduce tasks not specified. Estimated from input data size: 22\n",
    "In order to change the average load for a reducer (in bytes):\n",
    "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
    "In order to limit the maximum number of reducers:\n",
    "  set hive.exec.reducers.max=<number>\n",
    "In order to set a constant number of reducers:\n",
    "  set mapreduce.job.reduces=<number>\n",
    "Starting Job = job_1457314320880_0035, Tracking URL = http://dsm1:8088/proxy/application_1457314320880_0035/\n",
    "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1457314320880_0035\n",
    "Hadoop job information for Stage-1: number of mappers: 41; number of reducers: 22\n",
    "2016-03-28 16:18:22,893 Stage-1 map = 0%,  reduce = 0%\n",
    "2016-03-28 16:18:33,518 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 91.87 sec\n",
    "2016-03-28 16:18:36,662 Stage-1 map = 2%,  reduce = 0%, Cumulative CPU 329.92 sec\n",
    "2016-03-28 16:18:37,709 Stage-1 map = 3%,  reduce = 0%, Cumulative CPU 414.59 sec\n",
    "2016-03-28 16:18:39,809 Stage-1 map = 4%,  reduce = 0%, Cumulative CPU 542.4 sec\n",
    "2016-03-28 16:18:41,914 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 649.9 sec\n",
    "#\n",
    "# ...\n",
    "#\n",
    "2016-03-28 16:21:03,963 Stage-1 map = 97%,  reduce = 31%, Cumulative CPU 6457.47 sec\n",
    "2016-03-28 16:21:15,181 Stage-1 map = 98%,  reduce = 31%, Cumulative CPU 6506.44 sec\n",
    "2016-03-28 16:21:17,221 Stage-1 map = 100%,  reduce = 31%, Cumulative CPU 6517.32 sec\n",
    "2016-03-28 16:21:18,242 Stage-1 map = 100%,  reduce = 48%, Cumulative CPU 6523.4 sec\n",
    "2016-03-28 16:21:19,266 Stage-1 map = 100%,  reduce = 66%, Cumulative CPU 6550.58 sec\n",
    "2016-03-28 16:21:20,286 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6606.53 sec\n",
    "MapReduce Total cumulative CPU time: 0 days 1 hours 50 minutes 6 seconds 530 msec\n",
    "Ended Job = job_1457314320880_0035\n",
    "Launching Job 2 out of 2\n",
    "Number of reduce tasks not specified. Estimated from input data size: 1\n",
    "In order to change the average load for a reducer (in bytes):\n",
    "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
    "In order to limit the maximum number of reducers:\n",
    "  set hive.exec.reducers.max=<number>\n",
    "In order to set a constant number of reducers:\n",
    "  set mapreduce.job.reduces=<number>\n",
    "Starting Job = job_1457314320880_0036, Tracking URL = http://dsm1:8088/proxy/application_1457314320880_0036/\n",
    "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1457314320880_0036\n",
    "Hadoop job information for Stage-2: number of mappers: 6; number of reducers: 1\n",
    "2016-03-28 16:21:27,506 Stage-2 map = 0%,  reduce = 0%\n",
    "2016-03-28 16:21:32,631 Stage-2 map = 83%,  reduce = 0%, Cumulative CPU 13.05 sec\n",
    "2016-03-28 16:21:34,675 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 17.27 sec\n",
    "2016-03-28 16:21:39,792 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 21.84 sec\n",
    "MapReduce Total cumulative CPU time: 21 seconds 840 msec\n",
    "Ended Job = job_1457314320880_0036\n",
    "Copying data to local directory /home/cbutl002/leagueoflegends\n",
    "Copying data to local directory /home/cbutl002/leagueoflegends\n",
    "MapReduce Jobs Launched: \n",
    "Stage-Stage-1: Map: 41  Reduce: 22   Cumulative CPU: 6606.53 sec   HDFS Read: 5462381833 HDFS Write: 20436085 SUCCESS\n",
    "Stage-Stage-2: Map: 6  Reduce: 1   Cumulative CPU: 21.84 sec   HDFS Read: 20442565 HDFS Write: 10467897 SUCCESS\n",
    "Total MapReduce CPU Time Spent: 0 days 1 hours 50 minutes 28 seconds 370 msec\n",
    "OK\n",
    "Time taken: 205.624 seconds\n",
    "hive> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Muptiple Outputs at Once\n",
    "\n",
    "**Action:** write and execute a query to find out how many different subreddits are represented in the comment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the query I wrote\n",
    "SELECT COUNT(DISTINCT subreddit) AS num_subreddits FROM rc;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This query efficiently counts the number of unique subreddit threads by counting distinct values in the subreddit field from the rc data table and renaming it under the alias num_subreddits. It ran in just about 2 minutes, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input and output\n",
    "\n",
    "hive> select count(distinct subreddit) as subreddit_count from rc;              Query ID = cbutl002_20160328165252_89cfbeed-62ec-4922-b71d-f2330523ec3f\n",
    "Total jobs = 1\n",
    "Launching Job 1 out of 1\n",
    "Number of reduce tasks determined at compile time: 1\n",
    "In order to change the average load for a reducer (in bytes):\n",
    "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
    "In order to limit the maximum number of reducers:\n",
    "  set hive.exec.reducers.max=<number>\n",
    "In order to set a constant number of reducers:\n",
    "  set mapreduce.job.reduces=<number>\n",
    "Starting Job = job_1457314320880_0038, Tracking URL = http://dsm1:8088/proxy/application_1457314320880_0038/\n",
    "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1457314320880_0038\n",
    "Hadoop job information for Stage-1: number of mappers: 41; number of reducers: 1\n",
    "2016-03-28 16:52:53,314 Stage-1 map = 0%,  reduce = 0%\n",
    "2016-03-28 16:53:03,815 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 73.83 sec\n",
    "2016-03-28 16:53:05,928 Stage-1 map = 2%,  reduce = 0%, Cumulative CPU 221.8 sec\n",
    "2016-03-28 16:53:06,985 Stage-1 map = 3%,  reduce = 0%, Cumulative CPU 305.79 sec\n",
    "2016-03-28 16:53:08,030 Stage-1 map = 4%,  reduce = 0%, Cumulative CPU 377.36 sec\n",
    "2016-03-28 16:53:10,136 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 510.88 sec\n",
    "2016-03-28 16:53:11,188 Stage-1 map = 6%,  reduce = 0%, Cumulative CPU 559.62 sec\n",
    "2016-03-28 16:53:12,233 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 623.67 sec\n",
    "2016-03-28 16:53:14,373 Stage-1 map = 8%,  reduce = 0%, Cumulative CPU 723.99 sec\n",
    "# \n",
    "# ...\n",
    "#\n",
    "2016-03-28 16:54:53,758 Stage-1 map = 91%,  reduce = 23%, Cumulative CPU 5358.48 sec\n",
    "2016-03-28 16:54:54,779 Stage-1 map = 96%,  reduce = 23%, Cumulative CPU 5377.62 sec\n",
    "2016-03-28 16:54:55,799 Stage-1 map = 98%,  reduce = 29%, Cumulative CPU 5395.2 sec\n",
    "2016-03-28 16:54:56,824 Stage-1 map = 100%,  reduce = 29%, Cumulative CPU 5399.89 sec\n",
    "2016-03-28 16:54:58,864 Stage-1 map = 100%,  reduce = 69%, Cumulative CPU 5401.87 sec\n",
    "2016-03-28 16:54:59,888 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5402.94 sec\n",
    "MapReduce Total cumulative CPU time: 0 days 1 hours 30 minutes 2 seconds 940 msec\n",
    "Ended Job = job_1457314320880_0038\n",
    "MapReduce Jobs Launched: \n",
    "Stage-Stage-1: Map: 41  Reduce: 1   Cumulative CPU: 5402.94 sec   HDFS Read: 5462381833 HDFS Write: 6 SUCCESS\n",
    "Total MapReduce CPU Time Spent: 0 days 1 hours 30 minutes 2 seconds 940 msec\n",
    "OK\n",
    "47172\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the total number of unique subreddit threads is 47,172. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multiple outputs at once, take 2\n",
    "\n",
    "**Action:** ask hive to EXPLAIN these queries. Based on the explanation, would you expect one or the other to be faster? Test your expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hive> EXPLAIN select c.* from (select subreddit, count(*) as count from rc group by subreddit) c where c.count >10000;\n",
    "OK\n",
    "STAGE DEPENDENCIES:\n",
    "  Stage-1 is a root stage\n",
    "  Stage-0 depends on stages: Stage-1\n",
    "\n",
    "STAGE PLANS:\n",
    "  Stage: Stage-1\n",
    "    Map Reduce\n",
    "      Map Operator Tree:\n",
    "          TableScan\n",
    "            alias: rc\n",
    "            Statistics: Num rows: 54524134 Data size: 5452413440 Basic stats: COMPLETE Column stats: NONE\n",
    "            Select Operator\n",
    "              expressions: subreddit (type: string)\n",
    "              outputColumnNames: subreddit\n",
    "              Statistics: Num rows: 54524134 Data size: 5452413440 Basic stats: COMPLETE Column stats: NONE\n",
    "              Group By Operator\n",
    "                aggregations: count()\n",
    "                keys: subreddit (type: string)\n",
    "                mode: hash\n",
    "                outputColumnNames: _col0, _col1\n",
    "                Statistics: Num rows: 54524134 Data size: 5452413440 Basic stats: COMPLETE Column stats: NONE\n",
    "                Reduce Output Operator\n",
    "                  key expressions: _col0 (type: string)\n",
    "                  sort order: +\n",
    "                  Map-reduce partition columns: _col0 (type: string)\n",
    "                  Statistics: Num rows: 54524134 Data size: 5452413440 Basic stats: COMPLETE Column stats: NONE\n",
    "                  value expressions: _col1 (type: bigint)\n",
    "      Reduce Operator Tree:\n",
    "        Group By Operator\n",
    "          aggregations: count(VALUE._col0)\n",
    "          keys: KEY._col0 (type: string)\n",
    "          mode: mergepartial\n",
    "          outputColumnNames: _col0, _col1\n",
    "          Statistics: Num rows: 27262067 Data size: 2726206720 Basic stats: COMPLETE Column stats: NONE\n",
    "          Filter Operator\n",
    "            predicate: (_col1 > 10000) (type: boolean)\n",
    "            Statistics: Num rows: 9087355 Data size: 908735506 Basic stats: COMPLETE Column stats: NONE\n",
    "            Select Operator\n",
    "              expressions: _col0 (type: string), _col1 (type: bigint)\n",
    "              outputColumnNames: _col0, _col1\n",
    "              Statistics: Num rows: 9087355 Data size: 908735506 Basic stats: COMPLETE Column stats: NONE\n",
    "              File Output Operator\n",
    "                compressed: false\n",
    "                Statistics: Num rows: 9087355 Data size: 908735506 Basic stats: COMPLETE Column stats: NONE\n",
    "                table:\n",
    "                    input format: org.apache.hadoop.mapred.TextInputFormat\n",
    "                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n",
    "                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n",
    "\n",
    "  Stage: Stage-0\n",
    "    Fetch Operator\n",
    "      limit: -1\n",
    "      Processor Tree:\n",
    "        ListSink\n",
    "\n",
    "Time taken: 0.117 seconds, Fetched: 55 row(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hive> EXPLAIN select subreddit, count(*) from rc group by subreddit having count(*) > 10000;\n",
    "OK\n",
    "STAGE DEPENDENCIES:\n",
    "  Stage-1 is a root stage\n",
    "  Stage-0 depends on stages: Stage-1\n",
    "\n",
    "STAGE PLANS:\n",
    "  Stage: Stage-1\n",
    "    Map Reduce\n",
    "      Map Operator Tree:\n",
    "          TableScan\n",
    "            alias: rc\n",
    "            Statistics: Num rows: 54524134 Data size: 5452413440 Basic stats: COMPLETE Column stats: NONE\n",
    "            Select Operator\n",
    "              expressions: subreddit (type: string)\n",
    "              outputColumnNames: subreddit\n",
    "              Statistics: Num rows: 54524134 Data size: 5452413440 Basic stats: COMPLETE Column stats: NONE\n",
    "              Group By Operator\n",
    "                aggregations: count()\n",
    "                keys: subreddit (type: string)\n",
    "                mode: hash\n",
    "                outputColumnNames: _col0, _col1\n",
    "                Statistics: Num rows: 54524134 Data size: 5452413440 Basic stats: COMPLETE Column stats: NONE\n",
    "                Reduce Output Operator\n",
    "                  key expressions: _col0 (type: string)\n",
    "                  sort order: +\n",
    "                  Map-reduce partition columns: _col0 (type: string)\n",
    "                  Statistics: Num rows: 54524134 Data size: 5452413440 Basic stats: COMPLETE Column stats: NONE\n",
    "                  value expressions: _col1 (type: bigint)\n",
    "      Reduce Operator Tree:\n",
    "        Group By Operator\n",
    "          aggregations: count(VALUE._col0)\n",
    "          keys: KEY._col0 (type: string)\n",
    "          mode: mergepartial\n",
    "          outputColumnNames: _col0, _col1\n",
    "          Statistics: Num rows: 27262067 Data size: 2726206720 Basic stats: COMPLETE Column stats: NONE\n",
    "          Filter Operator\n",
    "            predicate: (_col1 > 10000) (type: boolean)\n",
    "            Statistics: Num rows: 9087355 Data size: 908735506 Basic stats: COMPLETE Column stats: NONE\n",
    "            Select Operator\n",
    "              expressions: _col0 (type: string), _col1 (type: bigint)\n",
    "              outputColumnNames: _col0, _col1\n",
    "              Statistics: Num rows: 9087355 Data size: 908735506 Basic stats: COMPLETE Column stats: NONE\n",
    "              File Output Operator\n",
    "                compressed: false\n",
    "                Statistics: Num rows: 9087355 Data size: 908735506 Basic stats: COMPLETE Column stats: NONE\n",
    "                table:\n",
    "                    input format: org.apache.hadoop.mapred.TextInputFormat\n",
    "                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n",
    "                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n",
    "\n",
    "  Stage: Stage-0\n",
    "    Fetch Operator\n",
    "      limit: -1\n",
    "      Processor Tree:\n",
    "        ListSink\n",
    "\n",
    "Time taken: 0.048 seconds, Fetched: 55 row(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what I can tell, it looks as though these two commands will run the two commands using more or less the same approaches.  If I had to choose which one I thought would run more efficiently, I would choose the second one because it has a more condensed command line.  I run them both below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hive> select c.* from (select subreddit, count(*) as count from rc group by subreddit) c where c.count >10000;\n",
    "Query ID = cbutl002_20160328175858_976e4b9c-be21-423b-a098-43181bd66707\n",
    "Total jobs = 1\n",
    "Launching Job 1 out of 1\n",
    "Number of reduce tasks not specified. Estimated from input data size: 22\n",
    "In order to change the average load for a reducer (in bytes):\n",
    "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
    "In order to limit the maximum number of reducers:\n",
    "  set hive.exec.reducers.max=<number>\n",
    "In order to set a constant number of reducers:\n",
    "  set mapreduce.job.reduces=<number>\n",
    "Starting Job = job_1457314320880_0040, Tracking URL = http://dsm1:8088/proxy/application_1457314320880_0040/\n",
    "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1457314320880_0040\n",
    "Hadoop job information for Stage-1: number of mappers: 41; number of reducers: 22\n",
    "2016-03-28 17:58:05,471 Stage-1 map = 0%,  reduce = 0%\n",
    "2016-03-28 17:58:15,897 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 73.83 sec\n",
    "2016-03-28 17:58:17,972 Stage-1 map = 2%,  reduce = 0%, Cumulative CPU 221.59 sec\n",
    "2016-03-28 17:58:19,011 Stage-1 map = 3%,  reduce = 0%, Cumulative CPU 300.77 sec\n",
    "#        \n",
    "# ... shortened for conciseness\n",
    "#\n",
    "2016-03-28 18:00:11,643 Stage-1 map = 100%,  reduce = 48%, Cumulative CPU 5435.98 sec\n",
    "2016-03-28 18:00:12,663 Stage-1 map = 100%,  reduce = 79%, Cumulative CPU 5468.25 sec\n",
    "2016-03-28 18:00:13,682 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5491.22 sec\n",
    "MapReduce Total cumulative CPU time: 0 days 1 hours 31 minutes 31 seconds 220 msec\n",
    "Ended Job = job_1457314320880_0040\n",
    "MapReduce Jobs Launched: \n",
    "Stage-Stage-1: Map: 41  Reduce: 22   Cumulative CPU: 5491.22 sec   HDFS Read: 5462381833 HDFS Write: 12077 SUCCESS\n",
    "Total MapReduce CPU Time Spent: 0 days 1 hours 31 minutes 31 seconds 220 msec\n",
    "OK\n",
    "AdoptMyVillager\t10815\n",
    "CasualConversation\t156316\n",
    "CollegeBasketball\t94296\n",
    "DarkSouls2\t62092\n",
    "DebateAnAtheist\t10703\n",
    "GlobalOffensive\t348153\n",
    "GrandTheftAutoV_PC\t12302\n",
    "#\n",
    "# ...\n",
    "#\n",
    "paradoxplaza\t13959\n",
    "politics\t220647\n",
    "programming\t46553\n",
    "steroids\t26183\n",
    "summonerswar\t50983\n",
    "truegaming\t15411\n",
    "woahdude\t35196\n",
    "Time taken: 136.854 seconds, Fetched: 715 row(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hive>  select subreddit, count(*) from rc group by subreddit having count(*) > 10000;\n",
    "Query ID = cbutl002_20160328180202_fde60ced-1398-4c8b-9c8d-5e0e98c17568\n",
    "Total jobs = 1\n",
    "Launching Job 1 out of 1\n",
    "Number of reduce tasks not specified. Estimated from input data size: 22\n",
    "In order to change the average load for a reducer (in bytes):\n",
    "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
    "In order to limit the maximum number of reducers:\n",
    "  set hive.exec.reducers.max=<number>\n",
    "In order to set a constant number of reducers:\n",
    "  set mapreduce.job.reduces=<number>\n",
    "Starting Job = job_1457314320880_0042, Tracking URL = http://dsm1:8088/proxy/application_1457314320880_0042/\n",
    "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1457314320880_0042\n",
    "Hadoop job information for Stage-1: number of mappers: 41; number of reducers: 22\n",
    "2016-03-28 18:02:26,652 Stage-1 map = 0%,  reduce = 0%\n",
    "2016-03-28 18:02:38,336 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 81.89 sec\n",
    "2016-03-28 18:02:41,489 Stage-1 map = 2%,  reduce = 0%, Cumulative CPU 203.66 sec\n",
    "2016-03-28 18:02:45,733 Stage-1 map = 3%,  reduce = 0%, Cumulative CPU 306.99 sec\n",
    "2016-03-28 18:02:48,925 Stage-1 map = 4%,  reduce = 0%, Cumulative CPU 400.56 sec\n",
    "2016-03-28 18:02:52,117 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 503.45 sec\n",
    "#\n",
    "# shortened again\n",
    "#\n",
    "2016-03-28 18:05:43,580 Stage-1 map = 99%,  reduce = 31%, Cumulative CPU 5861.12 sec\n",
    "2016-03-28 18:05:44,598 Stage-1 map = 99%,  reduce = 32%, Cumulative CPU 5861.38 sec\n",
    "2016-03-28 18:05:45,626 Stage-1 map = 100%,  reduce = 35%, Cumulative CPU 5865.83 sec\n",
    "2016-03-28 18:05:46,653 Stage-1 map = 100%,  reduce = 61%, Cumulative CPU 5886.66 sec\n",
    "2016-03-28 18:05:47,671 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5928.85 sec\n",
    "MapReduce Total cumulative CPU time: 0 days 1 hours 38 minutes 48 seconds 850 msec\n",
    "Ended Job = job_1457314320880_0042\n",
    "MapReduce Jobs Launched: \n",
    "Stage-Stage-1: Map: 41  Reduce: 22   Cumulative CPU: 5928.85 sec   HDFS Read: 5462381833 HDFS Write: 12077 SUCCESS\n",
    "Total MapReduce CPU Time Spent: 0 days 1 hours 38 minutes 48 seconds 850 msec\n",
    "OK\n",
    "AdoptMyVillager\t10815\n",
    "CasualConversation\t156316\n",
    "CollegeBasketball\t94296\n",
    "DarkSouls2\t62092\n",
    "DebateAnAtheist\t10703\n",
    "GlobalOffensive\t348153\n",
    "GrandTheftAutoV_PC\t12302\n",
    "Jokes\t53878\n",
    "#\n",
    "# ...\n",
    "#\n",
    "paradoxplaza\t13959\n",
    "politics\t220647\n",
    "programming\t46553\n",
    "steroids\t26183\n",
    "summonerswar\t50983\n",
    "truegaming\t15411\n",
    "woahdude\t35196\n",
    "Time taken: 210.98 seconds, Fetched: 715 row(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, a more efficient command line does not necessarily mean a more efficient computation considering the time that each command took.  The second command line took over one minute more time to compute than the first command line.  So, I had to rethink my answer. \n",
    "\n",
    "The less condensed version of the command line is more specific, giving the program less to work out computationally, enabling it to work moderately faster.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action:** execute this query, preferably just before a coffee break (it takes around 15-20 minutes to execute). Investigate the output (using hadoop fs -ls and hadoop fs -cat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hive> set hive.exec.max.dynamic.partitions = 1000;\n",
    "hive> set hive.exec.max.dynamic.partitions.pernode = 1000;\n",
    "hive> set hive.optimize.sort.dynamic.partition = true;\n",
    "hive> set hive.exec.dynamic.partition.mode = nonstrict;\n",
    "hive> insert overwrite table rcall partition (subreddit)\n",
    "    > select a.author, b.author, count(*), a.subreddit from \n",
    "    > rc a join rc b on a.name = b.parent_id\n",
    "    > join (select subreddit, count(*) as count from rc group by subreddit) c on a.subreddit = c.subreddit\n",
    "    > where c.count > 10000\n",
    "    > group by a.author, b.author, a.subreddit;\n",
    "Query ID = cbutl002_20160330134040_b7ac2117-702c-4568-9b31-df4c4fa25148\n",
    "Total jobs = 7\n",
    "Launching Job 1 out of 7\n",
    "Number of reduce tasks not specified. Estimated from input data size: 22\n",
    "In order to change the average load for a reducer (in bytes):\n",
    "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
    "In order to limit the maximum number of reducers:\n",
    "  set hive.exec.reducers.max=<number>\n",
    "In order to set a constant number of reducers:\n",
    "  set mapreduce.job.reduces=<number>\n",
    "Starting Job = job_1457314320880_0143, Tracking URL = http://dsm1:8088/proxy/application_1457314320880_0143/\n",
    "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1457314320880_0143\n",
    "Hadoop job information for Stage-7: number of mappers: 41; number of reducers: 22\n",
    "2016-03-30 13:40:58,217 Stage-7 map = 0%,  reduce = 0%\n",
    "2016-03-30 13:41:11,609 Stage-7 map = 1%,  reduce = 0%, Cumulative CPU 100.35 sec\n",
    "2016-03-30 13:41:16,760 Stage-7 map = 2%,  reduce = 0%, Cumulative CPU 265.37 sec\n",
    "2016-03-30 13:41:19,855 Stage-7 map = 3%,  reduce = 0%, Cumulative CPU 376.86 sec\n",
    "2016-03-30 13:41:22,936 Stage-7 map = 4%,  reduce = 0%, Cumulative CPU 475.68 sec\n",
    "2016-03-30 13:41:26,023 Stage-7 map = 5%,  reduce = 0%, Cumulative CPU 575.19 sec\n",
    "#\n",
    "# ...\n",
    "#\n",
    "2016-03-30 13:45:09,833 Stage-7 map = 97%,  reduce = 30%, Cumulative CPU 6315.76 sec\n",
    "2016-03-30 13:45:10,850 Stage-7 map = 97%,  reduce = 31%, Cumulative CPU 6321.28 sec\n",
    "2016-03-30 13:45:13,905 Stage-7 map = 99%,  reduce = 31%, Cumulative CPU 6339.63 sec\n",
    "2016-03-30 13:45:14,923 Stage-7 map = 100%,  reduce = 31%, Cumulative CPU 6345.67 sec\n",
    "2016-03-30 13:45:15,941 Stage-7 map = 100%,  reduce = 37%, Cumulative CPU 6347.71 sec\n",
    "2016-03-30 13:45:16,959 Stage-7 map = 100%,  reduce = 66%, Cumulative CPU 6373.12 sec\n",
    "2016-03-30 13:45:17,978 Stage-7 map = 100%,  reduce = 95%, Cumulative CPU 6412.99 sec\n",
    "2016-03-30 13:45:18,995 Stage-7 map = 100%,  reduce = 100%, Cumulative CPU 6419.03 sec\n",
    "MapReduce Total cumulative CPU time: 0 days 1 hours 46 minutes 59 seconds 30 msec\n",
    "Ended Job = job_1457314320880_0143\n",
    "Stage-1 is selected by condition resolver.\n",
    "Launching Job 2 out of 7\n",
    "Number of reduce tasks not specified. Estimated from input data size: 22\n",
    "In order to change the average load for a reducer (in bytes):\n",
    "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
    "In order to limit the maximum number of reducers:\n",
    "  set hive.exec.reducers.max=<number>\n",
    "In order to set a constant number of reducers:\n",
    "  set mapreduce.job.reduces=<number>\n",
    "Starting Job = job_1457314320880_0144, Tracking URL = http://dsm1:8088/proxy/application_1457314320880_0144/\n",
    "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1457314320880_0144\n",
    "Hadoop job information for Stage-1: number of mappers: 41; number of reducers: 22\n",
    "2016-03-30 13:45:25,170 Stage-1 map = 0%,  reduce = 0%\n",
    "2016-03-30 13:45:39,668 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 149.62 sec\n",
    "2016-03-30 13:45:43,769 Stage-1 map = 2%,  reduce = 0%, Cumulative CPU 298.84 sec\n",
    "2016-03-30 13:45:47,869 Stage-1 map = 3%,  reduce = 0%, Cumulative CPU 429.87 sec\n",
    "2016-03-30 13:45:50,941 Stage-1 map = 4%,  reduce = 0%, Cumulative CPU 524.47 sec\n",
    "2016-03-30 13:45:54,006 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 619.45 sec\n",
    "2016-03-30 13:45:57,092 Stage-1 map = 6%,  reduce = 0%, Cumulative CPU 714.84 sec\n",
    "2016-03-30 13:46:00,158 Stage-1 map = 7%,  reduce = 0%, Cumulative CPU 806.04 sec\n",
    "2016-03-30 13:46:03,226 Stage-1 map = 8%,  reduce = 0%, Cumulative CPU 900.66 sec\n",
    "#\n",
    "# ...\n",
    "#\n",
    "2016-03-30 13:50:59,026 Stage-1 map = 100%,  reduce = 82%, Cumulative CPU 7698.87 sec\n",
    "2016-03-30 13:51:00,052 Stage-1 map = 100%,  reduce = 84%, Cumulative CPU 7721.13 sec\n",
    "2016-03-30 13:51:01,072 Stage-1 map = 100%,  reduce = 88%, Cumulative CPU 7750.0 sec\n",
    "2016-03-30 13:51:02,090 Stage-1 map = 100%,  reduce = 90%, Cumulative CPU 7770.39 sec\n",
    "2016-03-30 13:51:03,107 Stage-1 map = 100%,  reduce = 94%, Cumulative CPU 7801.85 sec\n",
    "2016-03-30 13:51:04,124 Stage-1 map = 100%,  reduce = 97%, Cumulative CPU 7822.28 sec\n",
    "2016-03-30 13:51:06,157 Stage-1 map = 100%,  reduce = 99%, Cumulative CPU 7843.98 sec\n",
    "2016-03-30 13:51:07,176 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 7845.18 sec\n",
    "MapReduce Total cumulative CPU time: 0 days 2 hours 10 minutes 45 seconds 180 msec\n",
    "Ended Job = job_1457314320880_0144\n",
    "Stage-14 is selected by condition resolver.\n",
    "Stage-15 is filtered out by condition resolver.\n",
    "Stage-2 is filtered out by condition resolver.\n",
    "SLF4J: Class path contains multiple SLF4J bindings.\n",
    "SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: Found binding in [jar:file:/usr/local/apache-hive-0.14.0-bin/lib/hive-jdbc-0.14.0-standalone.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
    "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
    "Execution log at: /tmp/cbutl002/cbutl002_20160330134040_b7ac2117-702c-4568-9b31-df4c4fa25148.log\n",
    "2016-03-30 01:51:13\tStarting to launch local task to process map join;\tmaximum memory = 514850816\n",
    "2016-03-30 01:51:13\tDump the side-table for tag: 1 with group count: 715 into file: file:/tmp/cbutl002/d832fff0-b7a3-4a78-a81d-a67ef75a0d94/hive_2016-03-30_13-40-52_513_1119632409957654735-1/-local-10006/HashTable-Stage-8/MapJoin-mapfile21--.hashtable\n",
    "2016-03-30 01:51:13\tUploaded 1 File to: file:/tmp/cbutl002/d832fff0-b7a3-4a78-a81d-a67ef75a0d94/hive_2016-03-30_13-40-52_513_1119632409957654735-1/-local-10006/HashTable-Stage-8/MapJoin-mapfile21--.hashtable (20217 bytes)\n",
    "2016-03-30 01:51:13\tEnd of local task; Time Taken: 0.816 sec.\n",
    "Execution completed successfully\n",
    "MapredLocal task succeeded\n",
    "Launching Job 4 out of 7\n",
    "Number of reduce tasks is set to 0 since there's no reduce operator\n",
    "Starting Job = job_1457314320880_0145, Tracking URL = http://dsm1:8088/proxy/application_1457314320880_0145/\n",
    "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1457314320880_0145\n",
    "Hadoop job information for Stage-8: number of mappers: 8; number of reducers: 0\n",
    "2016-03-30 13:51:19,708 Stage-8 map = 0%,  reduce = 0%\n",
    "2016-03-30 13:51:31,979 Stage-8 map = 13%,  reduce = 0%, Cumulative CPU 69.12 sec\n",
    "2016-03-30 13:51:33,002 Stage-8 map = 34%,  reduce = 0%, Cumulative CPU 78.9 sec\n",
    "2016-03-30 13:51:34,028 Stage-8 map = 49%,  reduce = 0%, Cumulative CPU 92.34 sec\n",
    "2016-03-30 13:51:38,123 Stage-8 map = 55%,  reduce = 0%, Cumulative CPU 114.83 sec\n",
    "2016-03-30 13:51:39,146 Stage-8 map = 58%,  reduce = 0%, Cumulative CPU 118.28 sec\n",
    "2016-03-30 13:51:40,167 Stage-8 map = 73%,  reduce = 0%, Cumulative CPU 131.74 sec\n",
    "2016-03-30 13:51:44,261 Stage-8 map = 77%,  reduce = 0%, Cumulative CPU 149.61 sec\n",
    "2016-03-30 13:51:45,284 Stage-8 map = 80%,  reduce = 0%, Cumulative CPU 152.86 sec\n",
    "2016-03-30 13:51:46,306 Stage-8 map = 91%,  reduce = 0%, Cumulative CPU 162.98 sec\n",
    "2016-03-30 13:51:49,368 Stage-8 map = 97%,  reduce = 0%, Cumulative CPU 174.24 sec\n",
    "2016-03-30 13:51:50,387 Stage-8 map = 100%,  reduce = 0%, Cumulative CPU 174.89 sec\n",
    "MapReduce Total cumulative CPU time: 2 minutes 54 seconds 890 msec\n",
    "Ended Job = job_1457314320880_0145\n",
    "Launching Job 5 out of 7\n",
    "Number of reduce tasks not specified. Estimated from input data size: 6\n",
    "In order to change the average load for a reducer (in bytes):\n",
    "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
    "In order to limit the maximum number of reducers:\n",
    "  set hive.exec.reducers.max=<number>\n",
    "In order to set a constant number of reducers:\n",
    "  set mapreduce.job.reduces=<number>\n",
    "Starting Job = job_1457314320880_0146, Tracking URL = http://dsm1:8088/proxy/application_1457314320880_0146/\n",
    "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1457314320880_0146\n",
    "Hadoop job information for Stage-3: number of mappers: 7; number of reducers: 6\n",
    "2016-03-30 13:51:56,559 Stage-3 map = 0%,  reduce = 0%\n",
    "2016-03-30 13:52:07,798 Stage-3 map = 4%,  reduce = 0%, Cumulative CPU 56.42 sec\n",
    "2016-03-30 13:52:12,910 Stage-3 map = 23%,  reduce = 0%, Cumulative CPU 104.15 sec\n",
    "2016-03-30 13:52:13,931 Stage-3 map = 35%,  reduce = 0%, Cumulative CPU 111.49 sec\n",
    "2016-03-30 13:52:15,972 Stage-3 map = 50%,  reduce = 0%, Cumulative CPU 130.93 sec\n",
    "2016-03-30 13:52:17,196 Stage-3 map = 54%,  reduce = 0%, Cumulative CPU 138.76 sec\n",
    "2016-03-30 13:52:18,218 Stage-3 map = 56%,  reduce = 0%, Cumulative CPU 139.77 sec\n",
    "2016-03-30 13:52:20,262 Stage-3 map = 61%,  reduce = 0%, Cumulative CPU 158.18 sec\n",
    "2016-03-30 13:52:22,313 Stage-3 map = 71%,  reduce = 0%, Cumulative CPU 174.07 sec\n",
    "2016-03-30 13:52:24,355 Stage-3 map = 72%,  reduce = 0%, Cumulative CPU 177.29 sec\n",
    "2016-03-30 13:52:25,378 Stage-3 map = 81%,  reduce = 0%, Cumulative CPU 186.9 sec\n",
    "2016-03-30 13:52:27,418 Stage-3 map = 89%,  reduce = 5%, Cumulative CPU 196.69 sec\n",
    "2016-03-30 13:52:28,439 Stage-3 map = 94%,  reduce = 12%, Cumulative CPU 203.35 sec\n",
    "2016-03-30 13:52:29,463 Stage-3 map = 95%,  reduce = 15%, Cumulative CPU 206.12 sec\n",
    "2016-03-30 13:52:30,484 Stage-3 map = 95%,  reduce = 18%, Cumulative CPU 210.64 sec\n",
    "2016-03-30 13:52:31,508 Stage-3 map = 97%,  reduce = 25%, Cumulative CPU 221.05 sec\n",
    "2016-03-30 13:52:32,529 Stage-3 map = 97%,  reduce = 26%, Cumulative CPU 223.18 sec\n",
    "2016-03-30 13:52:33,550 Stage-3 map = 97%,  reduce = 28%, Cumulative CPU 223.3 sec\n",
    "2016-03-30 13:52:34,572 Stage-3 map = 99%,  reduce = 29%, Cumulative CPU 226.85 sec\n",
    "2016-03-30 13:52:35,592 Stage-3 map = 100%,  reduce = 29%, Cumulative CPU 227.86 sec\n",
    "2016-03-30 13:52:36,613 Stage-3 map = 100%,  reduce = 35%, Cumulative CPU 229.15 sec\n",
    "2016-03-30 13:52:37,636 Stage-3 map = 100%,  reduce = 54%, Cumulative CPU 233.59 sec\n",
    "2016-03-30 13:52:39,683 Stage-3 map = 100%,  reduce = 62%, Cumulative CPU 242.96 sec\n",
    "2016-03-30 13:52:40,706 Stage-3 map = 100%,  reduce = 71%, Cumulative CPU 262.82 sec\n",
    "2016-03-30 13:52:42,752 Stage-3 map = 100%,  reduce = 75%, Cumulative CPU 272.66 sec\n",
    "2016-03-30 13:52:43,773 Stage-3 map = 100%,  reduce = 78%, Cumulative CPU 282.57 sec\n",
    "2016-03-30 13:52:45,820 Stage-3 map = 100%,  reduce = 82%, Cumulative CPU 292.41 sec\n",
    "2016-03-30 13:52:46,843 Stage-3 map = 100%,  reduce = 86%, Cumulative CPU 302.34 sec\n",
    "2016-03-30 13:52:48,888 Stage-3 map = 100%,  reduce = 90%, Cumulative CPU 312.19 sec\n",
    "2016-03-30 13:52:49,911 Stage-3 map = 100%,  reduce = 93%, Cumulative CPU 322.03 sec\n",
    "2016-03-30 13:52:50,935 Stage-3 map = 100%,  reduce = 94%, Cumulative CPU 324.43 sec\n",
    "2016-03-30 13:52:51,955 Stage-3 map = 100%,  reduce = 97%, Cumulative CPU 331.03 sec\n",
    "2016-03-30 13:52:52,973 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 340.9 sec\n",
    "MapReduce Total cumulative CPU time: 5 minutes 40 seconds 900 msec\n",
    "Ended Job = job_1457314320880_0146\n",
    "Launching Job 6 out of 7\n",
    "Number of reduce tasks not specified. Estimated from input data size: 5\n",
    "In order to change the average load for a reducer (in bytes):\n",
    "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
    "In order to limit the maximum number of reducers:\n",
    "  set hive.exec.reducers.max=<number>\n",
    "In order to set a constant number of reducers:\n",
    "  set mapreduce.job.reduces=<number>\n",
    "Starting Job = job_1457314320880_0147, Tracking URL = http://dsm1:8088/proxy/application_1457314320880_0147/\n",
    "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1457314320880_0147\n",
    "Hadoop job information for Stage-4: number of mappers: 8; number of reducers: 5\n",
    "2016-03-30 13:52:59,135 Stage-4 map = 0%,  reduce = 0%\n",
    "2016-03-30 13:53:09,345 Stage-4 map = 13%,  reduce = 0%, Cumulative CPU 49.97 sec\n",
    "2016-03-30 13:53:10,368 Stage-4 map = 17%,  reduce = 0%, Cumulative CPU 67.65 sec\n",
    "2016-03-30 13:53:13,438 Stage-4 map = 24%,  reduce = 0%, Cumulative CPU 92.84 sec\n",
    "2016-03-30 13:53:14,459 Stage-4 map = 25%,  reduce = 0%, Cumulative CPU 93.56 sec\n",
    "2016-03-30 13:53:15,480 Stage-4 map = 61%,  reduce = 0%, Cumulative CPU 116.1 sec\n",
    "2016-03-30 13:53:16,501 Stage-4 map = 70%,  reduce = 0%, Cumulative CPU 120.16 sec\n",
    "2016-03-30 13:53:18,553 Stage-4 map = 85%,  reduce = 0%, Cumulative CPU 136.25 sec\n",
    "2016-03-30 13:53:19,576 Stage-4 map = 87%,  reduce = 0%, Cumulative CPU 139.9 sec\n",
    "2016-03-30 13:53:20,597 Stage-4 map = 92%,  reduce = 10%, Cumulative CPU 145.68 sec\n",
    "2016-03-30 13:53:21,620 Stage-4 map = 96%,  reduce = 15%, Cumulative CPU 153.67 sec\n",
    "2016-03-30 13:53:22,641 Stage-4 map = 98%,  reduce = 20%, Cumulative CPU 159.59 sec\n",
    "2016-03-30 13:53:23,662 Stage-4 map = 100%,  reduce = 25%, Cumulative CPU 166.2 sec\n",
    "2016-03-30 13:53:24,687 Stage-4 map = 100%,  reduce = 26%, Cumulative CPU 166.51 sec\n",
    "2016-03-30 13:53:25,709 Stage-4 map = 100%,  reduce = 28%, Cumulative CPU 167.98 sec\n",
    "2016-03-30 13:53:26,731 Stage-4 map = 100%,  reduce = 53%, Cumulative CPU 176.97 sec\n",
    "2016-03-30 13:53:27,754 Stage-4 map = 100%,  reduce = 60%, Cumulative CPU 181.01 sec\n",
    "2016-03-30 13:53:28,778 Stage-4 map = 100%,  reduce = 68%, Cumulative CPU 184.79 sec\n",
    "2016-03-30 13:53:29,800 Stage-4 map = 100%,  reduce = 70%, Cumulative CPU 192.93 sec\n",
    "2016-03-30 13:53:30,822 Stage-4 map = 100%,  reduce = 71%, Cumulative CPU 194.97 sec\n",
    "2016-03-30 13:53:31,848 Stage-4 map = 100%,  reduce = 73%, Cumulative CPU 197.74 sec\n",
    "2016-03-30 13:53:33,891 Stage-4 map = 100%,  reduce = 74%, Cumulative CPU 203.42 sec\n",
    "2016-03-30 13:53:34,912 Stage-4 map = 100%,  reduce = 79%, Cumulative CPU 207.52 sec\n",
    "2016-03-30 13:53:35,933 Stage-4 map = 100%,  reduce = 80%, Cumulative CPU 209.22 sec\n",
    "2016-03-30 13:53:36,955 Stage-4 map = 100%,  reduce = 81%, Cumulative CPU 210.75 sec\n",
    "2016-03-30 13:53:37,979 Stage-4 map = 100%,  reduce = 84%, Cumulative CPU 215.06 sec\n",
    "2016-03-30 13:53:40,021 Stage-4 map = 100%,  reduce = 85%, Cumulative CPU 217.76 sec\n",
    "2016-03-30 13:53:41,041 Stage-4 map = 100%,  reduce = 87%, Cumulative CPU 221.7 sec\n",
    "2016-03-30 13:53:42,061 Stage-4 map = 100%,  reduce = 89%, Cumulative CPU 223.47 sec\n",
    "2016-03-30 13:53:43,082 Stage-4 map = 100%,  reduce = 90%, Cumulative CPU 225.42 sec\n",
    "2016-03-30 13:53:44,102 Stage-4 map = 100%,  reduce = 93%, Cumulative CPU 230.29 sec\n",
    "2016-03-30 13:53:45,125 Stage-4 map = 100%,  reduce = 95%, Cumulative CPU 232.15 sec\n",
    "2016-03-30 13:53:46,145 Stage-4 map = 100%,  reduce = 96%, Cumulative CPU 234.85 sec\n",
    "2016-03-30 13:53:47,165 Stage-4 map = 100%,  reduce = 97%, Cumulative CPU 237.88 sec\n",
    "2016-03-30 13:53:49,205 Stage-4 map = 100%,  reduce = 98%, Cumulative CPU 239.92 sec\n",
    "2016-03-30 13:53:51,247 Stage-4 map = 100%,  reduce = 99%, Cumulative CPU 242.93 sec\n",
    "2016-03-30 13:53:56,346 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 245.92 sec\n",
    "MapReduce Total cumulative CPU time: 4 minutes 5 seconds 920 msec\n",
    "Ended Job = job_1457314320880_0147\n",
    "Loading data to table default.rcall partition (subreddit=null)\n",
    "Failed with exception Unable to move source hdfs://dsm1:9000/tmp/hive/cbutl002/d832fff0-b7a3-4a78-a81d-a67ef75a0d94/hive_2016-03-30_13-40-52_513_1119632409957654735-1/-ext-10000/subreddit=whowouldwin to destination hdfs://dsm1:9000/user/hive/warehouse/rcall/subreddit=whowouldwin\n",
    "FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask\n",
    "MapReduce Jobs Launched: \n",
    "Stage-Stage-7: Map: 41  Reduce: 22   Cumulative CPU: 6419.03 sec   HDFS Read: 5462381833 HDFS Write: 21971 SUCCESS\n",
    "Stage-Stage-1: Map: 41  Reduce: 22   Cumulative CPU: 7845.18 sec   HDFS Read: 5462381833 HDFS Write: 1651077054 SUCCESS\n",
    "Stage-Stage-8: Map: 8   Cumulative CPU: 174.89 sec   HDFS Read: 1651083824 HDFS Write: 1370794299 SUCCESS\n",
    "Stage-Stage-3: Map: 7  Reduce: 6   Cumulative CPU: 340.9 sec   HDFS Read: 1370808994 HDFS Write: 1039448342 SUCCESS\n",
    "Stage-Stage-4: Map: 8  Reduce: 5   Cumulative CPU: 247.04 sec   HDFS Read: 1039465556 HDFS Write: 508900635 SUCCESS\n",
    "Total MapReduce CPU Time Spent: 0 days 4 hours 10 minutes 27 seconds 40 msec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was unable to get this code to complete.  Each time I tried it, several of the jobs ran successfully, until one of hte tasks failed.  Each time, the error message read, \"Failed with exception Unable to move source hdfs://dsm1:9000/tmp/hive/cbutl002/d832fff0-b7a3-4a78-a81d-a67ef75a0d94/hive_2016-03-30_13-40-52_513_1119632409957654735-1/-ext-10000/subreddit=whowouldwin to destination hdfs://dsm1:9000/user/hive/warehouse/rcall/subreddit=whowouldwin\n",
    "FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask\" So, I looked this up online.  I found the following link: http://stackoverflow.com/questions/12786441/cant-import-load-data-to-hive-why This explained that the issue was likely due to user permission settings, and that there are a few workarounds.  The workarounds largely included disabling permissions, which I cannot do, creating new databases in Hive, so I wouldn't be able to access the Hadoop data, and some authorization methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Subreddit graph analytics\n",
    "**Action:** try this on a few of the subreddits. Are there any common users? How would you go about getting the top 10 commenters in a subreddit, or all those above 90% of the maximum connectedness value for that subreddit?\n",
    "\n",
    " \n",
    "I decided to pull data from the comedywriting, politics, and boston subreddits.  I originally wanted to use the centralcoast subreddit, since I went to college in the area, but the file ended up being so small that it continually crashed my kernel.  I enjoy reading about comedy writers and their work, I like to read political and social news, and I am from Boston, so they seemed like interesting subreddits to explore.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hive> insert overwrite local directory '/home/cbutl002/rcboston         \n",
    "    > '\n",
    "    > row format delimited fields terminated by '\\t'                    \n",
    "    > select a.author, b.author, count(*) from                          \n",
    "    > (select * from rc where subreddit = 'boston') a                   \n",
    "    > join\n",
    "    > (select * from rc where subreddit = 'boston') b                   \n",
    "    > on a.name = b.parent_id\n",
    "    > group by a.author, b.author;\n",
    "\n",
    "hive> insert overwrite local directory '/home/cbutl002/rccomedywriting'\n",
    "    > row format delimited fields terminated by '\\t'                    \n",
    "    > select a.author, b.author, count(*) from                          \n",
    "    > (select * from rc where subreddit = 'comedywriting') a                   \n",
    "    > join\n",
    "    > (select * from rc where subreddit = 'comedywriting') b                   \n",
    "    > on a.name = b.parent_id\n",
    "    > group by a.author, b.author;\n",
    "\n",
    "hive> insert overwrite local directory '/home/cbutl002/rcpolitics'\n",
    "    > row format delimited fields terminated by '\\t'                    \n",
    "    > select a.author, b.author, count(*) from                          \n",
    "    > (select * from rc where subreddit = 'politics') a                   \n",
    "    > join\n",
    "    > (select * from rc where subreddit = 'politics') b                   \n",
    "    > on a.name = b.parent_id\n",
    "    > group by a.author, b.author;\n",
    "    \n",
    "[cbutl002@dsm1 ~]$ cat rcboston/* > rcboston.ncol\n",
    "[cbutl002@dsm1 ~]$ cat rccomedywriting/* > rccomedywriting.ncol\n",
    "[cbutl002@dsm1 ~]$ cat rcpolitics/* > rcpolitics.ncol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# modified to replicate Kate's forum post about the mdegree variable\n",
    "def most_connected_user(subreddit, file):\n",
    "    # parse the graph\n",
    "    G = igraph.Graph.Read_Ncol(file)\n",
    "    # delete the vertex corresponding to the [deleted] pseudo-user\n",
    "    G.delete_vertices(G.vs['name'].index('[deleted]'))\n",
    "    # compute the geometric mean of the in- and out-degree for each vertex\n",
    "    mdegree = list(map(lambda x,y: sqrt(x*y), G.indegree(), G.outdegree()))\n",
    "    i = mdegree.index(max(mdegree))\n",
    "    print(\"most connected commenter in \",subreddit,\": \",G.vs[i]['name'],\"(indegree: \",G.indegree(i),\"outdegree: \",G.outdegree(i),\")\")\n",
    "\n",
    "# this code prints the top ten most connected users in ascending order based on their geometric means\n",
    "def connected_users(subreddit,file):\n",
    "    # parse the graph\n",
    "    G = igraph.Graph.Read_Ncol(file)\n",
    "    # delete the vertex corresponding to the [deleted] pseudo-user\n",
    "    G.delete_vertices(G.vs['name'].index('[deleted]'))\n",
    "    # compute the geometric mean of the in- and out-degree for each vertex and do list comprehension to create tuples\n",
    "    # of geometric mean and user name\n",
    "    connected_users = [(sqrt(v.indegree()*v.outdegree()), v['name']) for v in G.vs() ]\n",
    "    print(sorted(connected_users, key = lambda users: users[0])[-10:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most connected commenter in  boston :  Mitch_from_Boston (indegree:  212 outdegree:  244 )\n",
      "[(51.526692111953004, 'Baron_Von_D'), (51.768716422179139, 'nOrthSC'), (54.671747731346578, 'VictorHugosBaseball'), (62.801273872430329, 'crabonfibre'), (63.482280992415511, 'cpxh'), (63.576725301009333, 'highlander311'), (64.498061986388393, 'NightStreet'), (103.46980235798269, 'Boston_Jason'), (148.91608375189028, 'itsonlyastrongbuzz'), (227.437903613272, 'Mitch_from_Boston')]\n"
     ]
    }
   ],
   "source": [
    "boston_MCU = most_connected_user(\"boston\",\"rcboston.ncol\")\n",
    "boston_CU = connected_users(\"boston\",\"rcboston.ncol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code seems valid based on the names of the users.  Several of them mention Boston or references to Boston, so they must be pretty huge Boston fans!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most connected commenter in  comedywriting :  jimhodgson (indegree:  3 outdegree:  6 )\n",
      "[(1.4142135623730951, 'levilarrington'), (1.4142135623730951, 'uberbeard'), (1.4142135623730951, 'loetz'), (2.0, 'DSPR'), (2.0, 'summerain92'), (2.0, 'Squirming_Coil'), (2.4494897427831779, 'Just_Joey'), (3.1622776601683795, 'Bill_Murray_Movies'), (3.4641016151377544, 'BigBlackCoke'), (4.2426406871192848, 'jimhodgson')]\n"
     ]
    }
   ],
   "source": [
    "comedywriting_MCU = most_connected_user(\"comedywriting\",\"rccomedywriting.ncol\")\n",
    "comedywriting_CU = connected_users(\"comedywriting\",\"rccomedywriting.ncol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, there is a user called Bill_Murray_Movies, so these users must appreciate comedy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most connected commenter in  politics :  SpinningHead (indegree:  571 outdegree:  465 )\n",
      "[(196.15809950139709, 'flantabulous'), (198.01010075246163, 'WaterOfForgetfulness'), (226.34045153264142, 'ben1204'), (242.14871463627472, 'guitarist_classical'), (262.26132006073635, 'sagan_drinks_cosmos'), (269.09849497906896, 'jpe77'), (269.81475126464085, 'Flameon3k'), (342.75355578024278, 'JumpingJazzJam'), (349.28498393145964, 'Brother_tempus'), (515.28147647669232, 'SpinningHead')]\n"
     ]
    }
   ],
   "source": [
    "centralcoast_MCU = most_connected_user(\"politics\",\"rcpolitics.ncol\")\n",
    "centralcoast_CU = connected_users(\"politics\",\"rcpolitics.ncol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 08b: Graph Algorithms in Spark\n",
    "\n",
    "I will:\n",
    "\n",
    "-> load and appropriately partition a graphical dataset into Spark\n",
    "\n",
    "-> run built-in graphical algorithms on that graphical dataset\n",
    "\n",
    "-> write a simple graphical program to compute a graph invariant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "\n",
    "**Action:** start the Spark scala shell, and enter these statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> import org.apache.spark._\n",
    "import org.apache.spark._\n",
    "\n",
    "scala> import org.apache.spark.graphx._\n",
    "import org.apache.spark.graphx._\n",
    "\n",
    "scala> import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "scala> val commentsFile = \"hdfs:///user/hive/warehouse/rcall/subreddit=ukpolitics\"\n",
    "commentsFile: String = hdfs:///user/hive/warehouse/rcall/subreddit=ukpolitics\n",
    "\n",
    "scala> val comments = sc.textFile(commentsFile).map(line => { val l = line.split(\"\\t\")\n",
    "     |   (l(0), l(1), l(2).toInt) }).filter { case (from, to, count) => from != \"[deleted]\" && to != \"[deleted]\" }\n",
    "16/03/29 19:38:27 INFO storage.MemoryStore: ensureFreeSpace(89176) called with curMem=0, maxMem=278302556\n",
    "16/03/29 19:38:27 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 87.1 KB, free 265.3 MB)\n",
    "16/03/29 19:38:27 INFO storage.MemoryStore: ensureFreeSpace(20000) called with curMem=89176, maxMem=278302556\n",
    "16/03/29 19:38:27 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 19.5 KB, free 265.3 MB)\n",
    "16/03/29 19:38:27 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:36983 (size: 19.5 KB, free: 265.4 MB)\n",
    "16/03/29 19:38:27 INFO spark.SparkContext: Created broadcast 0 from textFile at <console>:37\n",
    "comments: org.apache.spark.rdd.RDD[(String, String, Int)] = MapPartitionsRDD[3] at filter at <console>:38\n",
    "\n",
    "scala> // an RDD of (String, Long) (mapping names to vertexIDs)\n",
    "\n",
    "scala> val vertices = comments.flatMap(x => Seq(x._1, x._2)).distinct.zipWithIndex\n",
    "16/03/29 19:38:53 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
    "16/03/29 19:38:53 INFO spark.SparkContext: Starting job: zipWithIndex at <console>:39\n",
    "16/03/29 19:38:53 INFO scheduler.DAGScheduler: Registering RDD 5 (distinct at <console>:39)\n",
    "16/03/29 19:38:53 INFO scheduler.DAGScheduler: Got job 0 (zipWithIndex at <console>:39) with 1 output partitions (allowLocal=false)\n",
    "16/03/29 19:38:53 INFO scheduler.DAGScheduler: Final stage: ResultStage 1(zipWithIndex at <console>:39)\n",
    "16/03/29 19:38:53 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
    "16/03/29 19:38:53 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
    "16/03/29 19:38:53 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at distinct at <console>:39), which has no missing parents\n",
    "16/03/29 19:38:53 INFO storage.MemoryStore: ensureFreeSpace(4672) called with curMem=109176, maxMem=278302556\n",
    "16/03/29 19:38:53 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.6 KB, free 265.3 MB)\n",
    "16/03/29 19:38:53 INFO storage.MemoryStore: ensureFreeSpace(2459) called with curMem=113848, maxMem=278302556\n",
    "16/03/29 19:38:53 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.4 KB, free 265.3 MB)\n",
    "16/03/29 19:38:53 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:36983 (size: 2.4 KB, free: 265.4 MB)\n",
    "16/03/29 19:38:53 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:874\n",
    "16/03/29 19:38:53 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at distinct at <console>:39)\n",
    "16/03/29 19:38:53 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 2 tasks\n",
    "16/03/29 19:38:53 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, ANY, 1434 bytes)\n",
    "16/03/29 19:38:53 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, ANY, 1434 bytes)\n",
    "16/03/29 19:38:53 INFO executor.Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
    "16/03/29 19:38:53 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
    "16/03/29 19:38:53 INFO rdd.HadoopRDD: Input split: hdfs://dsm1:9000/user/hive/warehouse/rcall/subreddit=ukpolitics/000002_0:0+204686\n",
    "16/03/29 19:38:53 INFO rdd.HadoopRDD: Input split: hdfs://dsm1:9000/user/hive/warehouse/rcall/subreddit=ukpolitics/000002_0:204686+204687\n",
    "16/03/29 19:38:53 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
    "16/03/29 19:38:53 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
    "16/03/29 19:38:53 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
    "16/03/29 19:38:53 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
    "16/03/29 19:38:53 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
    "16/03/29 19:38:54 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2002 bytes result sent to driver\n",
    "16/03/29 19:38:54 INFO executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 2002 bytes result sent to driver\n",
    "16/03/29 19:38:54 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 532 ms on localhost (1/2)\n",
    "16/03/29 19:38:54 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 542 ms on localhost (2/2)\n",
    "16/03/29 19:38:54 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
    "16/03/29 19:38:54 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (distinct at <console>:39) finished in 0.550 s\n",
    "16/03/29 19:38:54 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
    "16/03/29 19:38:54 INFO scheduler.DAGScheduler: running: Set()\n",
    "16/03/29 19:38:54 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 1)\n",
    "16/03/29 19:38:54 INFO scheduler.DAGScheduler: failed: Set()\n",
    "16/03/29 19:38:54 INFO scheduler.DAGScheduler: Missing parents for ResultStage 1: List()\n",
    "16/03/29 19:38:54 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at distinct at <console>:39), which is now runnable\n",
    "16/03/29 19:38:54 INFO storage.MemoryStore: ensureFreeSpace(2624) called with curMem=116307, maxMem=278302556\n",
    "16/03/29 19:38:54 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 2.6 KB, free 265.3 MB)\n",
    "16/03/29 19:38:54 INFO storage.MemoryStore: ensureFreeSpace(1576) called with curMem=118931, maxMem=278302556\n",
    "16/03/29 19:38:54 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1576.0 B, free 265.3 MB)\n",
    "16/03/29 19:38:54 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:36983 (size: 1576.0 B, free: 265.4 MB)\n",
    "16/03/29 19:38:54 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:874\n",
    "16/03/29 19:38:54 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at distinct at <console>:39)\n",
    "16/03/29 19:38:54 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks\n",
    "16/03/29 19:38:54 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, PROCESS_LOCAL, 1165 bytes)\n",
    "16/03/29 19:38:54 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 2)\n",
    "16/03/29 19:38:54 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks\n",
    "16/03/29 19:38:54 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
    "16/03/29 19:38:54 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 2). 924 bytes result sent to driver\n",
    "16/03/29 19:38:54 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 117 ms on localhost (1/1)\n",
    "16/03/29 19:38:54 INFO scheduler.DAGScheduler: ResultStage 1 (zipWithIndex at <console>:39) finished in 0.117 s\n",
    "16/03/29 19:38:54 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
    "16/03/29 19:38:54 INFO scheduler.DAGScheduler: Job 0 finished: zipWithIndex at <console>:39, took 0.746481 s\n",
    "vertices: org.apache.spark.rdd.RDD[(String, Long)] = ZippedWithIndexRDD[8] at zipWithIndex at <console>:39\n",
    "\n",
    "scala> // an RDD of Edge(Long, Long, Int) (declaring edges with Int weights)\n",
    "\n",
    "scala> val edges = comments.map { case (fromName, toName, count) => (fromName, (toName, count)) }.\n",
    "     |   join(vertices).\n",
    "     |   map { case (fromName, ((toName, count), fromID)) => (toName, (fromID, count)) }.\n",
    "     |   join(vertices).\n",
    "     |   map { case (toName, ((fromID, count), toID)) => Edge(fromID, toID, count) }\n",
    "edges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]] = MapPartitionsRDD[17] at map at <console>:45\n",
    "\n",
    "scala> // vertices have String names, Edges have Int weights\n",
    "\n",
    "scala> val cGraph = Graph.apply[String, Int](\n",
    "     |   // vertices as (VertexID, String)\n",
    "     |   vertices.map(_.swap),\n",
    "     |   // edges as Edge(VertexID, VertexID, Int)\n",
    "     |   edges)\n",
    "16/03/29 19:39:28 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on localhost:36983 in memory (size: 1576.0 B, free: 265.4 MB)\n",
    "16/03/29 19:39:28 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on localhost:36983 in memory (size: 2.4 KB, free: 265.4 MB)\n",
    "cGraph: org.apache.spark.graphx.Graph[String,Int] = org.apache.spark.graphx.impl.GraphImpl@3db14f97"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Degrees\n",
    "\n",
    "**Action:** who are the top 10 most-connected commenters? Is there a difference between those replied to (use inDegrees) and those commenting (outDegrees)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> cGraph.vertices.join(cGraph.degrees).sortBy(-_._2._2).take(10)\n",
    "res0: Array[(org.apache.spark.graphx.VertexId, (String, Int))] = Array((2165,(Lolworth,445)), \n",
    "                                                                       (436,(ProfessorZ00M,355)), \n",
    "                                                                       (43,(Pallas_,318)), \n",
    "                                                                       (618,(NotSoBlue_,306)), \n",
    "                                                                       (257,(naturalredditor,300)), \n",
    "                                                                       (135,(cbzoiav,273)), \n",
    "                                                                       (652,(blue_dice,267)), \n",
    "                                                                       (346,(Tarambor,258)), \n",
    "                                                                       (1075,(DevilishRogue,235)), \n",
    "                                                                       (1217,(Orcnick,233)))\n",
    "scala> cGraph.vertices.join(cGraph.inDegrees).sortBy(-_._2._2).take(10)\n",
    "res1: Array[(org.apache.spark.graphx.VertexId, (String, Int))] = Array((2165,(Lolworth,247)), \n",
    "                                                                       (135,(cbzoiav,165)), \n",
    "                                                                       (618,(NotSoBlue_,153)), \n",
    "                                                                       (436,(ProfessorZ00M,149)), \n",
    "                                                                       (257,(naturalredditor,141)), \n",
    "                                                                       (43,(Pallas_,137)), \n",
    "                                                                       (346,(Tarambor,132)), \n",
    "                                                                       (652,(blue_dice,130)), \n",
    "                                                                       (1075,(DevilishRogue,125)), \n",
    "                                                                       (147,(KarmaUK,124)))\n",
    "scala> cGraph.vertices.join(cGraph.outDegrees).sortBy(-_._2._2).take(10)\n",
    "res2: Array[(org.apache.spark.graphx.VertexId, (String, Int))] = Array((436,(ProfessorZ00M,206)), \n",
    "                                                                       (2165,(Lolworth,198)), \n",
    "                                                                       (43,(Pallas_,181)), \n",
    "                                                                       (257,(naturalredditor,159)), \n",
    "                                                                       (618,(NotSoBlue_,153)), \n",
    "                                                                       (652,(blue_dice,137)), \n",
    "                                                                       (1217,(Orcnick,136)), \n",
    "                                                                       (1389,(whencanistop,128)), \n",
    "                                                                       (346,(Tarambor,126)), \n",
    "                                                                       (8,(InDefens,118)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three types of degrees, including most wholly connected commenters, the commenters most replied to, and the commenters that reply the most do vary a bit.  There are several commenters who appear in all three lists, such as Lolworth, ProfessorZ00M, Pallas_, NotSoBlue, naturalredditor, and Tarambor.  In addition, there are some commenters who only appear in 2 out of the three lists.  However, it seems rare for a commenter to only be on one list.  This reflects the different types of social media users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PageRank\n",
    "\n",
    "Are the top 10 most-connected commenters (by whatever measure) the same as the top 10 by pagerank score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pagerank algorithm largely recommends the same commenters, when based purely on connectedness.  However, they are not all entirely the same.  There is a bit of variance reflecting the different most connected commenter results from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vertices pagerank\n",
    "scala> val pr = cGraph.pageRank(0.001)\n",
    "scala> cGraph.vertices.join(pr.vertices).sortBy(-_._2._2).take(10)\n",
    "res3: Array[(org.apache.spark.graphx.VertexId, (String, Double))] = Array((2165,(Lolworth,20.046831662787532)), \n",
    "                                                                          (135,(cbzoiav,14.710551766611191)), \n",
    "                                                                          (346,(Tarambor,13.334499114659476)), \n",
    "                                                                          (618,(NotSoBlue_,13.22773859322327)), \n",
    "                                                                          (436,(ProfessorZ00M,12.615928707493454)), \n",
    "                                                                          (652,(blue_dice,11.560905793795932)), \n",
    "                                                                          (43,(Pallas_,11.239068504011321)), \n",
    "                                                                          (1075,(DevilishRogue,11.070010461565314)), \n",
    "                                                                          (257,(naturalredditor,10.985268570129387)), \n",
    "                                                                          (147,(KarmaUK,10.958410290585968)))\n",
    "# degrees pagerank\n",
    "scala> cGraph.vertices.join(pr.degrees).sortBy(-_._2._2).take(10)\n",
    "res4: Array[(org.apache.spark.graphx.VertexId, (String, Int))] = Array((2165,(Lolworth,445)), \n",
    "                                                                       (436,(ProfessorZ00M,355)), \n",
    "                                                                       (43,(Pallas_,318)), \n",
    "                                                                       (618,(NotSoBlue_,306)), \n",
    "                                                                       (257,(naturalredditor,300)), \n",
    "                                                                       (135,(cbzoiav,273)), \n",
    "                                                                       (652,(blue_dice,267)), \n",
    "                                                                       (346,(Tarambor,258)), \n",
    "                                                                       (1075,(DevilishRogue,235)), \n",
    "                                                                       (1217,(Orcnick,233)))\n",
    "# in degrees pagerank\n",
    "scala> cGraph.vertices.join(pr.inDegrees).sortBy(-_._2._2).take(10)\n",
    "res5: Array[(org.apache.spark.graphx.VertexId, (String, Int))] = Array((2165,(Lolworth,247)), \n",
    "                                                                       (135,(cbzoiav,165)), \n",
    "                                                                       (618,(NotSoBlue_,153)), \n",
    "                                                                       (436,(ProfessorZ00M,149)), \n",
    "                                                                       (257,(naturalredditor,141)), \n",
    "                                                                       (43,(Pallas_,137)), \n",
    "                                                                       (346,(Tarambor,132)), \n",
    "                                                                       (652,(blue_dice,130)), \n",
    "                                                                       (1075,(DevilishRogue,125)), \n",
    "                                                                       (147,(KarmaUK,124)))\n",
    "# out degrees pagerank    \n",
    "scala> cGraph.vertices.join(pr.outDegrees).sortBy(-_._2._2).take(10)\n",
    "res6: Array[(org.apache.spark.graphx.VertexId, (String, Int))] = Array((436,(ProfessorZ00M,206)), \n",
    "                                                                       (2165,(Lolworth,198)), \n",
    "                                                                       (43,(Pallas_,181)), \n",
    "                                                                       (257,(naturalredditor,159)), \n",
    "                                                                       (618,(NotSoBlue_,153)), \n",
    "                                                                       (652,(blue_dice,137)), \n",
    "                                                                       (1217,(Orcnick,136)), \n",
    "                                                                       (1389,(whencanistop,128)), \n",
    "                                                                       (346,(Tarambor,126)), \n",
    "                                                                       (8,(InDefens,118)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Connected Components\n",
    "\n",
    "GraphX provides an implementation of a connected components algorithm, which labels each vertex by the lowest vertexID it can reach; then each connected component is the set of vertices with the same label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val cc = cGraph.connectedComponents\n",
    "scala> cc.vertices.map(_.swap).countByKey\n",
    "res7: scala.collection.Map[org.apache.spark.graphx.VertexId,Long] = Map(0 -> 2166, 174 -> 2, 853 -> 2, 179 -> 2, \n",
    "                                                                        133 -> 2, 849 -> 3, 64 -> 2, 59 -> 2, \n",
    "                                                                        915 -> 2, 278 -> 2, 624 -> 2, 1004 -> 2, \n",
    "                                                                        1870 -> 2)\n",
    "scala> val sg = cGraph.mask(cc.subgraph(vpred = (vid, attr) => attr == 0))\n",
    "sg: org.apache.spark.graphx.Graph[String,Int] = org.apache.spark.graphx.impl.GraphImpl@3888cebe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Shortest path\n",
    "\n",
    "The following is an illustration on how to implement a graph algorithm using the message-passing Pregel paradigm: in particular, this computes the shortest path length to all vertices in the graph from a single source vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val sourceID = 0L\n",
    "sourceID: Long = 0\n",
    "\n",
    "scala> val dists = sg.mapVertices((id, _) => if (id == sourceID) 0 else Int.MaxValue  - 1).\n",
    "     |   pregel(Int.MaxValue, Int.MaxValue, EdgeDirection.Either)(\n",
    "     |     // vertex program\n",
    "     |     (id, dist, newDist) => math.min(dist, newDist),\n",
    "     |     // send message (need to handle bidirectionality ourselves)\n",
    "     |     triplet => {\n",
    "     |       (if (triplet.srcAttr + 1 < triplet.dstAttr) {\n",
    "     |         Iterator((triplet.dstId, triplet.srcAttr + 1))\n",
    "     |       } else {\n",
    "     |         Iterator.empty\n",
    "     |       }) ++\n",
    "     |       (if (triplet.dstAttr + 1 < triplet.srcAttr) {\n",
    "     |         Iterator((triplet.srcId, triplet.dstAttr + 1))\n",
    "     |       } else {\n",
    "     |         Iterator.empty\n",
    "     |       })\n",
    "     |     },\n",
    "     |     // merge message\n",
    "     |     (a, b) => math.min(a, b)\n",
    "     |   )\n",
    "dists: org.apache.spark.graphx.Graph[Int,Int] = org.apache.spark.graphx.impl.GraphImpl@4918455c\n",
    "\n",
    "scala> dists.vertices.map(_.swap).countByKey\n",
    "res8: scala.collection.Map[Int,Long] = Map(0 -> 1, 5 -> 398, 1 -> 1, 6 -> 16, 2 -> 4, 7 -> 1, 3 -> 308, 4 -> 1437)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
